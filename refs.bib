@inproceedings{Hinton1995BayesianLF,
    title={Bayesian learning for neural networks},
    author={Geoffrey E. Hinton and R. Neal},
    year={1995}
}


@article{MacKay1992,
    author = {MacKay, David J. C.},
    title = "{A Practical Bayesian Framework for Backpropagation Networks}",
    journal = {Neural Computation},
    volume = {4},
    number = {3},
    pages = {448-472},
    year = {1992},
    month = {05},
    abstract = "{A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1992.4.3.448},
    url = {https://doi.org/10.1162/neco.1992.4.3.448},
    eprint = {https://direct.mit.edu/neco/article-pdf/4/3/448/812348/neco.1992.4.3.448.pdf},
}

@article{Mozer1989AFB,
        title={A Focused Backpropagation Algorithm for Temporal Pattern Recognition},
        author={Michael C. Mozer},
        journal={Complex Systems},
        year={1989},
        volume={3}
}

@article{Bengio1994LearningLD,
        title={Learning long-term dependencies with gradient descent is difficult},
        author={Yoshua Bengio and Patrice Y. Simard and Paolo Frasconi},
        journal={IEEE transactions on neural networks},
        year={1994},
        volume={5 2},
        pages={157-66}
}

@article{Hochreiter1997LongSM,
        title={Long Short-Term Memory},
        author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
        journal={Neural Computation},
        year={1997},
        volume={9},
        pages={1735-1780}
}

@inproceedings{Cho2014LearningPR,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
	      van Merri{\"e}nboer, Bart  and
	      Gulcehre, Caglar  and
	      Bahdanau, Dzmitry  and
	      Bougares, Fethi  and
	      Schwenk, Holger  and
	      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}


@inproceedings{Chung2015NIPS,
    author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {A Recurrent Latent Variable Model for Sequential Data},
    url = {https://proceedings.neurips.cc/paper/2015/file/b618c3210e934362ac261db280128c22-Paper.pdf},
    volume = {28},
    year = {2015}
}

@article{Fortunato2017bayesian,
    title={Bayesian recurrent neural networks},
    author={Fortunato, Meire and Blundell, Charles and Vinyals, Oriol},
    journal={arXiv preprint arXiv:1704.02798},
    year={2017}
}

@inproceedings{Hinton1993,
    author = {Hinton, Geoffrey E. and van Camp, Drew},
    title = {Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights},
    year = {1993},
    isbn = {0897916115},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/168304.168306},
    doi = {10.1145/168304.168306},
    booktitle = {Proceedings of the Sixth Annual Conference on Computational Learning Theory},
    pages = {5–13},
    numpages = {9},
    location = {Santa Cruz, California, USA},
    series = {COLT '93}
}

@inproceedings{Blundell2015,
    author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
    title = {Weight Uncertainty in Neural Networks},
    year = {2015},
    publisher = {JMLR.org},
    abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm
		for learning a probability distribution on the weights of a neural network, called
		Bayes by Backprop. It regularises the weights by minimising a compression cost, known
		as the variational free energy or the expected lower bound on the marginal likelihood.
		We show that this principled kind of regularisation yields comparable performance
		to dropout on MNIST classification. We then demonstrate how the learnt uncertainty
		in the weights can be used to improve generalisation in non-linear regression problems,
		and how this weight uncertainty can be used to drive the exploration-exploitation
		trade-off in reinforcement learning.},
    booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
    pages = {1613–1622},
    numpages = {10},
    location = {Lille, France},
    series = {ICML'15}
}


@InProceedings{Gal2016,
    title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
    author = {Gal, Yarin and Ghahramani, Zoubin},
    booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
    pages = {1050--1059},
    year = {2016},
    editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
    volume = {48},
    series = {Proceedings of Machine Learning Research},
    address = {New York, New York, USA},
    month = {20--22 Jun},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v48/gal16.pdf},
    url = {https://proceedings.mlr.press/v48/gal16.html},
    abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}


@inproceedings{Gal2016NIPS,
    author = {Gal, Yarin and Ghahramani, Zoubin},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},
    url = {https://proceedings.neurips.cc/paper/2016/file/076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf},
    volume = {29},
    year = {2016}
}

@article{Zhu2017DeepAC,
    title={Deep and Confident Prediction for Time Series at Uber},
    author={Lingxue Zhu and Nikolay Pavlovich Laptev},
    journal={2017 IEEE International Conference on Data Mining Workshops (ICDMW)},
    year={2017},
    pages={103-110}
}

@InProceedings{Vandal2018,
    title = {Prediction and Uncertainty Quantification of Daily Airport Flight Delays},
    author = {Vandal, Thomas and Livingston, Max and Piho, Camen and Zimmerman, Sam},
    booktitle = {Proceedings of The 4th International Conference on Predictive Applications and APIs},
    pages = {45--51},
    year = {2018},
    editor = {Hardgrove, Claire and Dorard, Louis and Thompson, Keiran},
    volume = {82},
    series = {Proceedings of Machine Learning Research},
    month = {24--25 Oct},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v82/vandal18a/vandal18a.pdf},
    url = {https://proceedings.mlr.press/v82/vandal18a.html},
    abstract = {One in four commercial airline flights is delayed, inconveniencing travelers and causing large financial losses for carriers. The ability to accurately predict delays would make travelers’ lives easier and save airlines money. In this work, we approach the problem of predicting flight delays using a Variational Long Short-Term Memory (LSTM) model. The model is trained to predict aggregate daily delays for U.S. airports using a combination of continuous and discrete variables, including weather, airport characteristics, and congestion. Monte Carlo Dropout, a Bayesian Deep Learning technique based on variational inference, is incorporated to provide planners with a well-calibrated prediction interval. We show that our Variational LSTM results in an average median absolute error of 5.8 minutes per day across 123 airports in the United States. Moreover, results show that predictive uncertainty is well explained through a calibration analysis.}
}

@article{Wen2020UncertaintyQI,
    title={Uncertainty quantification in molecular simulations with dropout neural network potentials},
    author={Mingjian Wen and E. Tadmor},
    journal={npj Computational Materials},
    year={2020},
    volume={6},
    pages={1-10}
}

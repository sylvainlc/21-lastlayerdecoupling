\documentclass{article}
% Packages
\usepackage{amsmath} % Math environments
\usepackage{amsfonts, dsfont} % Math fonts
\usepackage{authblk} % Author and affiliations
\usepackage{hyperref} % Links
\usepackage{graphicx} % Include graphics
\usepackage{apalike} % Reference styling
\usepackage{booktabs}

% Commands
\providecommand{\keywords}[1]{\textbf{\textit{Index terms---}} #1}

\title{Rebuttal for Reviewer 2163}

\author{Max Cohen}
\affil{Samovar, T\'el\'ecom SudParis, CITI, TIPIC, Institut Polyechnique de Paris}
\date{}

\begin{document}
\maketitle

We would like to start by thanking the reviewer for their thorough reading of our paper.
We took note of their remarks, and attempted to improve the clarity and precision of our work as best as possible.

In order to expand on our validation experiments
\footnote{We would like to highlight that although our original dataset contains a limited number of samples, each one represents a week's worth of hourly data for 10 different time series.}
, we benchmarked our model on a public dataset.
We chose the \textit{Appliances energy prediction Data Set}\footnote{https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction} from the UCI Machine Learning Repository, which contains indoor temperature and humidity monitoring for over four months, along with various weather variables (outdoor temperature, pressure, wind speed).
We compared our model with the LSTM Dropout on the indoor temperature forecasting task, and reported our results in Table~\ref{tab:comparison}, which comfort our first experiment.
As advised by the reviewer, computation times for each training epoch are reported, showing that our model is slower, without being prohibitive in practice.

The famously expensive computations of Sequential Monte Carlo (SMC) sampling is one of the main motivation for our paper, which we address by decoupling SMC computations from the rest of the model:
\begin{itemize}
	\item \textbf{training}: we were able to reach reasonable training times by separately training the model's backbone from the last uncertainty quantification layer.
	      This way, expensive SMC computations are limited to finetuning only a few parameters.
	      Additionally, we aim at implementing recent advances in SMC that allow smoothing with a linear computational complexity, see [].
	\item \textbf{inference}: for the forecasting task presented in our paper, we only need to propagate multiple particles at the same time, which is very efficient thanks to the inherent parallel implementation of deep learning frameworks.
\end{itemize}


We agree that this was badly stated and thank the reviewer for reporting this inaccuracy. However, this is not the motivation of our paper. We address the overconfidence of current uncertainty estimation methods such as LSTM Dropout.
- The assumption and motivation stated in this paper are both incorrect. The authors mentioned RNNs models suffer from overconfidence when evaluated outside their training observation. This is not a problem unique to RNN; any data-driven model suffer from inability to generalize to out-of domain examples. A well-trained neural network model can interpolation between data seen during training (in-domain) but extrapolation is a challenging problem. I would not expect a neural network to perform any better when evaluated outside their training data distribution.

\begin{table}[htpb]
	\centering
	\caption{Comparison between LSTM Dropout and our SMC model on the new \texttt{energy} dataset.}
	\label{tab:comparison}
	\begin{tabular}{lllll}
		\toprule
		             & PICP            & MPIW    & MAE             & Training Epoch \\
		\toprule
		SMCL (ours)  & $\textbf{0.80}$ & $0.101$ & $0.84$          & 15s            \\
		MCD $p=0.15$ & $0.23$          & $0.56$  & $\textbf{0.82}$ & \textbf{3s}    \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{itemize}
	\item Reformuler les motivations pour expliquer clairement l'intérêt des modèles d'état ; souligner la différence entre généralisation et overconfidence.
	\item Le remarcier pour les bruits multimodals ; expliquer qu'on peut s'adapter à n'importe quel bruit en changeant les équations
\end{itemize}

\bibliographystyle{apalike}
\bibliography{references}
\end{document}

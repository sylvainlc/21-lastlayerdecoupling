% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx,xcolor}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\graphicspath{ {./images/}  }

\title{Last layer state space model}
%
% Single address.
% ---------------
\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
\address{Author Affiliation(s)}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%

\begin{document}
\maketitle

\begin{abstract}
	As sequential neural architectures become deeper and more complex, estimating the uncertainty of their predictions is ever so challenging.
	Efforts in quantifying uncertainty often rely on specific training procedures, and bear additional computational costs due to the dimensionality of such models.
	In this paper, we demonstrate how uncertainty can be estimated on top of an existing and trained neural network, combining a state space-based last  layer and  Sequential Monte Carlo methods. This approach allows to separate representation learning and uncertainty quantification. We apply our proposed methodology for the estimation of air quality in office buildings, through the hourly prediction of \ensuremath{\mathrm{CO_2}} and humidity levels.
	Our model accounts for the noisy data structure, due to unknown or unavailable variables (occupancy of the building, manual ventilation, etc.), and is able to provide confidence intervals on predictions.
\end{abstract}

\begin{keywords}
Recurrent neural networks, Uncertainty quantification, Sequantial Monte Carlo.
\end{keywords}

\section{Introduction}
\label{sec:intro}
% We believe a deeper understanding of \ensuremath{\mathrm{CO_2}} variations can assist in regulating and reducing HVAC consumption, while improving well being.

Recurrent Neural Networks (RNN) were first introduced as an efficient and convenient architecture to address short time dependencies problems \cite{Mozer1989AFB}.
They have been consistently improved to develop longer term memory, and optimize their implementations \cite{Bengio1994LearningLD,Hochreiter1997LongSM,Cho2014LearningPR}.
Current deep learning framework allows stacking arbitrary high number of recurrent layers, whose parameters are estimated by gradient descent through an automated differentiation procedure.

These recurrent architectures are able to approximate complex nonlinear time series, but suffer from overconfidence when evaluated outside of their training observations. Bayesian statistics aim at mitigating this drawback by providing uncertainty estimation \cite{Hinton1995BayesianLF,MacKay1992}.

Several architectures inspired by variational inference emerged by considering latent states as random variables and approximating the posterior.
The authors of \cite{Chung2015NIPS} built on a traditional RNN architecture by modelling temporal dependencies between these latent random states.
Results presented in \cite{Fortunato2017bayesian} yield improved performances when considering local gradient information for computing the posterior.
Similarly, introducing random variables directly into a neural network's weights can mitigate overconfidence, as demonstrated by the authors of \cite{Hinton1993}.
In \cite{Blundell2015}, weights are considered as distributions instead of vector values, allowing the network to make more reasonable predictions about unseen data.

Variational methods display encouraging results for modelling uncertainty in existing models, but require significant alteration of the model, as well as it's training mechanism.
In contrast, Monte Carlo Dropout (MC Dropout) methods offer to capture uncertainty by adding a single non parametric Dropout layer during both training and evaluation tasks, producing variable predictions from a single trained model, see \cite{Gal2016}.
These results were later extended to recurrent architectures in \cite{Gal2016NIPS}.
In the following years, MC Dropout methods have been applied in many industrial fields, such as anomaly detection (\cite{Zhu2017DeepAC}), flight delay prediction (\cite{Vandal2018}) or molecular simulations (\cite{Wen2020UncertaintyQI}).
Alternatively, ensemble methods consists in training distinct networks to obtain a combined prediction, as shown in \cite{Pearce2018}.
However, these frequentist approaches fail to guarantee proper calibration of the model, as highlighted by \cite{ashukha2020pitfalls}, and suffer various limitations, see \cite{Fong2020}.

In an effort to provide an alternative strategy with limited computation overhead, \cite{Brosse2020OnLA} suggests splitting training in two stages: representation learning and uncertainty estimation.
Their experiments indicate that the latter should be limited to the last layers of the model, as introducing randomness in the entire network increases complexity without offering much better results.

However, the benchmarked methods for uncertainty estimation cannot be applied in a time series context, as latent states are dependant.
Instead, we explore Sequential Monte Carlo methods, which were already successfully combined with recurrent neural network to tackle variable sequential problems, see \cite{Ma2020}.
We turn to \cite{Martin2020TheMC} for an example using more complex neural architectures, such as Transformer.
Particle filters have also proven reliable in other fields, such as presented in \cite{Liu2020LSTMPF} for object tracking.

In this paper, we introduce a new method for uncertainty estimation combining high expressivity, quality uncertainty estimations and ease of training.
Our proposed architecture is composed of an arbitrary sequential model, followed by a decoupled state space model layer.
The former can be fitted through a traditional gradient descent iteration, while the latter is trained using Sequential Monte Carlo methods.
Our uncertainty estimation layer can thus be built on top of an existing, already trained model.

\section{Last layer decoupling}
\label{sec:decoupling}

\subsection{Proposed model}%
\label{sub:proposed_architecture}

Our proposed model can be decomposed in two decoupled blocks.
An input model $h_\varphi$ is responsible for extracting relevant features from the input time series.
Its final hidden states are fed to a Sequential Monte Carlo Layer (SMCL), based on a plain RNN, for uncertainty estimation.

In the following, for any sequence $(a_m,\ldots, a_n)$ with $n\geq m$, we use the short-hand notation $a_{m:n} = (a_m,\ldots, a_n)$. Let $T\ge 1$ be a given time horizon. We consider the regression task associated with observations $Y_{1:T}$ and inputs $U_{1:T}$, and denote by $X_{1:T}$ the hidden states of the model. For all $t\geq 1$ consider the following model,
\begin{equation*}
	\left\{
	\begin{aligned}
		\widetilde U_t & = h_\varphi(U_t)                        & \text{input model}       \\
		X_t        & = g_\theta(X_{t-1}, \widetilde U_t, \eta_t) & \text{state model}       \\
		Y_t        & = f_\theta(X_t, \epsilon_t)             & \text{observation model,} \\
	\end{aligned}
	\right.
\end{equation*}
where $(\eta_t)_{t\geq 1}$ and $(\epsilon_t)_{t\geq 1}$ are two sequences of i.i.d. centered Gaussain random variables with covariance matrices $\Sigma_x$ and $\Sigma_y$.

\subsection{Input model}%
\label{sub:input_model}
Extracting high-level features for the initial inputs can be done efficiently using several multi-layer neural network architectures.  In this paper, we chose a multi-layered GRU network such that each hidden state is computed as follows, for all $1 \leq \ell \leq L$ and all $1 \leq t \leq T$,
$$
	\tilde U^\ell_t = k_\varphi^\ell(\widetilde U^{\ell-1}_t, \widetilde U^\ell_{t-1})\,,
$$
where the first layer is assimilated to the input vectors, $\widetilde U_t^0 = U_t$, and  $\widetilde U^\ell_0 = 0$.
This model is trained by disabling the noise addition in the SMCL functions $g_\theta$ and $f_\theta$, and considering the entire model as a deterministic neural network.
Through a traditional gradient descent, we then obtain estimations for the input model weights $\varphi$, as well as a pretraining for the SMCL layer.

\subsection{Uncertainty estimation}%
\label{sub:uncertainty_estimation}

We account for uncertainty in the pretrained state and observation models by considering hidden states as random variables, and estimating their parameters.
Consequently, we can no longer integrate over their possible values, and computation of the gradient becomes intractable.
Instead, we leverage Fisher's identity for an expression involving the posterior conditional expectation.

\begin{align}
	\nabla \log p_\theta(Y_{1:T}) = \mathbb{E}_\theta \left[ \nabla\log p_\theta(X_{1:T}, Y_{1:T}) | Y_{1:T} \right]
	\label{eq:grad_ll}
\end{align}

Sequential Monte Carlo methods aim at approximating this posterior by a set of $N$ weighted random trajectories.
At each time step $t$, these particles $\xi^{1:N}_t$ are associated likelihood weights $\omega^{1:N}_t$, given the previous observation $y_{t-1}$, in order to propagate only the most likely candidates, see \ref{algo:particle_filter}.
For any function $h$, we can approximate the conditional expectation as follows:
\begin{align*}
	\mathbb{E}_\theta \left[ h(X_{1:T}) | Y_{1:T} \right] = \sum_{i=1}^N \omega_T^i h(\xi^i_{1:T})
\end{align*}

We develop \ref{eq:grad_ll} to obtain an explicit form of the loss function ; its gradient is an approximation for the likelihood's gradient, and can be computed by deep learning frameworks through automated differentiation.
\begin{align*}
	\mathbb{J}(\theta) & = \log |\Sigma_x| + \log |\Sigma_y|                                                                                                        \\
	                   & + \frac{1}{T}\sum_{k=0}^T \sum_{i=1}^N \omega^i (y_k - f_\theta(\xi_k^i))' \Sigma_y^{-1} (y_k - f_\theta(\xi_k^i))                         \\
	                   & + \frac{1}{T}\sum_{k=1}^T \sum_{i=1}^N \omega^i (\xi_k^i - g_\theta(\xi_{k-1}^i, u_k))'\Sigma_x^{-1}(\xi_k^i - g_\theta(\xi_{k-1}^i, u_k))
\end{align*}

During the estimation of $\theta$, the input model is frozen, i.e. we stop gradient computation for $\varphi$. Both $\Sigma_x$ and $\Sigma_y$ are updated by an explicit EM, as gradient descent for these parameters proved unstable.

\begin{algorithm}
	\KwIn{$\theta, y_{1:T}, \tilde u_{1:T}$}
	\KwOut{$\xi_{1:T}^{1:N}$}
	$\xi_0^i \gets \eta^i$\;
	\For{$t \gets 0$ \KwTo $T$}{
		$\omega_t^i \gets \varphi_{y_t, \Sigma_y}(f_\theta(\xi_t^i))$\;
		$I_{t+1}^i \sim \mathbb{P}(I_{t+1}^i=j) = \omega_t^j$\;
		$\xi_{t+1}^i \gets g_\theta(\xi_t^{I_{t+1}^i}, \tilde u_{t+1}, \eta^i)$\;
	}
	\caption{Particle filter. $\varphi_{\mu, \Sigma}$ is the multivariate normal density function with mean $\mu$ and covariance matrix $\Sigma$.}
	\label{algo:particle_filter}
\end{algorithm}

\section{Experiments}
\label{sec:exp}

\subsection{Training}%
\label{sub:training}
Both training of the input model and the SMCL were performed on an air quality database, split into training (2020) and validation (January - XXX 2021) sets.
Each is composed of week-long hourly records associating weather data (humidity, temperature, irradiance), as well as prior building knowledge such as modeled occupancy, with observed \ensuremath{\mathrm{CO_2}} and humidity levels.
Trainings were conducted using the PyTorch framework, on a single GPU.

\subsection{Visualizations}%
\label{sub:visualizations}
Our model is able to perform uncertainty estimation by propagating particles.
In Figure~\ref{fig:filter_k+1}, we plot each particle's associated prediction, given the observation at the previous time step, on a validation sample.
The confidence intervals displayed increase whenever the average prediction strays from the observation.

Although predictions given the previous time step in quite performant, its application could be limited, as building management decisions are not usually hourly based.
We seize this opportunity to explore longer range filtering.
Since particles can no longer be resampled, we approximate their propagation.
In Figure~\ref{fig:filter_k+24}, we develop particle trajectories, using observations, for 72 time steps (3 days).
The second part of the graph represent their propagation ; as we model noise in the latent space, the confidence intervals grow steadily before converging in size.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=\linewidth]{filter_kp1_hum.png}
	\includegraphics[width=\linewidth]{filter_kp1_co2.png}
	\caption{Prediction of humidity (top) and CO2 (bottom) given previous observations. Insert explanation about boxplots.}%
	\label{fig:filter_k+1}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=\linewidth]{filter_kp24_hum.png}
	\includegraphics[width=\linewidth]{filter_kp24_co2.png}
	\caption{Prediction of humidity (top) and CO2 (bottom) given observation ($t<72$) and without ($t > 72$).}%
	\label{fig:filter_k+24}
\end{figure}

Visualizations listed in priority order
\begin{enumerate}
	\item Prediction at t+1: plot t+1 predictions as boxplots, along with the mean and the observations.
	\item Prediction at t+k
	\item Compare weights evolution with classic finetuning
	\item Smoothed predictions: Smoothed particles trajectories' associated prediction: sample particles from the posterior, smooth the trajectories, apply the model function, and plot the results as a interval.
\end{enumerate}

\subsection{Evaluations}%
\label{sub:evaluations}

Evaluations listed in priority order
\begin{enumerate}
	\item Compare confidence interval with MC-dropout model (soit LSTM dropout classique, soit en enlevant la derniere couche)
	\item Compare with ARIMA/classic stat model
	\item Compare MSE (average on particles) with classic finetuning: sample particles for half a week. For the second half, observations are not available at all ; thus we either take the mean of the predictions associated with each particles, or sample from the observation model and take the mean.
	\item Compare linear SMC with kalman filter, sample under the gaussian law, plot boxplot
\end{enumerate}


% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
%\begin{figure}[htb]

%\begin{minipage}[b]{1.0\linewidth}
%  \centering
%  \centerline{\includegraphics[width=8.5cm]{image1}}
%%  \vspace{2.0cm}
%  \centerline{(a) Result 1}\medskip
%\end{minipage}
%%
%\begin{minipage}[b]{.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image3}}
%%  \vspace{1.5cm}
%  \centerline{(b) Results 3}\medskip
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image4}}
%%  \vspace{1.5cm}
%  \centerline{(c) Result 4}\medskip
%\end{minipage}
%%
%\caption{Example of placing a figure with experimental results.}
%\label{fig:res}
%%
%\end{figure}


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs.bib}

\end{document}

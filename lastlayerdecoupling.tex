\documentclass[journal]{IEEEtran}

% Packages
\usepackage{amsmath} % Math environments
\usepackage{amsfonts, dsfont} % Math fonts
\usepackage{hyperref} % Links
\usepackage{graphicx} % Include graphics
\usepackage{booktabs,xcolor}
\usepackage[ruled,vlined]{algorithm2e}
\graphicspath{ {./images/} }

\title{Last layer state space model for representation learning and uncertainty quantification}

\author{Max Cohen, Maurice Charbit and Sylvain Le Corff
	\thanks{This work was supported by grants from RÃ©gion Ile-de-France.}
	\thanks{Max Cohen and Sylvain Le Corff are with with Samovar, T\'el\'ecom SudParis, CITI, TIPIC, Institut Polyechnique de Paris, (e-mail: max.cohen@telecom-sudparis.eu).}
	\thanks{Maurice Charbit is with Accenta, Boulogne-Billancourt (e-mail: maurice.charbit@accenta.fr).}}

\begin{document}
\maketitle
\begin{abstract}
	As sequential neural architectures become deeper and more complex, uncertainty estimation is more and more challenging.
	Efforts in quantifying uncertainty often rely on specific training procedures, and bear additional computational costs due to the dimensionality of such models.
	In this letter, we propose to decompose a classification or regression task in two steps: a representation learning stage to learn low-dimensional states, and a state space model for uncertainty estimation.
	This approach allows to separate representation learning and design of generative models.
	We demonstrate how predictive distributions can be estimated on top of an existing and trained neural network, by adding a state space-based last layer whose parameters are estimated with Sequential Monte Carlo methods.
	We apply our proposed methodology to the hourly estimation of Electricity Transformer Oil temperature, a publicly benchmarked dataset.
	Our model accounts for the noisy data structure, due to unknown or unavailable variables, and is able to provide confidence intervals on predictions.
\end{abstract}

\begin{IEEEkeywords}
	Recurrent neural networks, Representation learning, Uncertainty quantification, Sequential Monte Carlo.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
% We believe a deeper understanding of \ensuremath{\mathrm{CO_2}} variations can assist in regulating and reducing HVAC consumption, while improving well being.

Recurrent Neural Networks (RNN) were first introduced as an efficient and convenient architecture to address short time dependencies problems.
They have been consistently improved to develop longer term memory, and optimize their implementations \cite{Bengio1994LearningLD,Hochreiter1997LongSM}. %,Cho2014LearningPR}.
Current deep learning frameworks allow stacking arbitrary high number of recurrent layers, whose parameters are estimated by gradient descent through automated differentiation procedures, as shown in \cite{Graves2013SpeechRecognition}.
However, many critical applications, such as medical diagnosis or drug design discovery, require not only accurate predictions, but a good estimate of their uncertainty (\cite{Crowson2016AssessingCalibration, Mervin2020UncertaintyQuantification}).
Fostering the dissemination of deep learning-based algorithms to such fields requires to design new approaches for uncertainty quantification.

In this context, Bayesian statistics are able to approximate the distributions of future observations and to provide uncertainty estimation \cite{Hinton1995BayesianLF}.
Several architectures inspired by Variational Inference (VI, see \cite{Jordan2004AnIT}) emerged by considering latent states as random variables and approximating their posterior distribution.
The authors of \cite{Chung2015NIPS,10.5555/3157096.3157343} built on a traditional recurrent architecture by modelling temporal dependencies between these latent random states. Results presented in \cite{Fortunato2017bayesian} yield improved performances when considering local gradient information for computing the posterior.


In \cite{Blundell2015}, the authors considered weights as random variables and proposed approximations of their posterior distributions allowing more robust predictions. Such Bayesian neural networks have been proposed and studied in a variety of works, see for instance \cite{hernandez2015probabilistic,khan2018fast,teye2018bayesian}. However, these methods are computationally intensive for high dimensional models and we do not have statistical guarantees on their ability to capture the target  posterior distribution, see \cite{NEURIPS2020_b6dfd418}.

%Variational methods display encouraging results for modelling uncertainty in existing models, but require significant alteration of the model, as well as its training mechanism.
Monte Carlo Dropout (MC Dropout) methods offer to capture uncertainty by leveraging Dropout during both training and evaluation tasks, producing variable predictions from a single trained recurrent model, see \cite{Gal2016NIPS}.
In the recent years, MC Dropout methods have been applied in many industrial fields, such as flight delay prediction \cite{Vandal2018} or molecular simulations \cite{Wen2020UncertaintyQI}.
Alternatively, ensemble methods consist in training distinct networks to obtain a combined prediction, as shown in \cite{Pearce2018}.
However, these frequentist approaches fail to guarantee proper calibration of the model, as highlighted by \cite{ashukha2020pitfalls}, and suffer various limitations, see \cite{Fong2020}.

In an effort to provide an alternative strategy with limited computation overhead, \cite{Brosse2020OnLA} suggests splitting training in two stages to solve classification problems for independent data: representation learning and uncertainty estimation. The two steps proceed as follows: (i) the algorithm first trains a deep classifier to obtain accurate task-dependent representations of the data, and then (ii) ensemble models are trained using these representations and the output. Their experiments indicate that last-layer algorithms outperform baseline networks and that a single last layer is an appealing trade-off between computational cost and uncertainty quantification. However, this method is restricted to independent and identically distributed data and cannot be directly applied to time series.


Inspired by \cite{Brosse2020OnLA}, we propose a last layer approach to split uncertainty quantification from representation learning, in the context of dependent data. This new method for uncertainty estimation combines high expressivity, quality uncertainty estimations and ease of training. Our proposed architecture is composed of an arbitrary sequential model, followed by a decoupled state space model layer. Using a state space model in the last layer allows to introduce complex predictive distributions for the observations based on task-dependent latent data. However, because the loglikelihood of the observations is not available explicitly in such a setting, the second stage training requires using approximate sampling methods. In this letter, we explore Sequential Monte Carlo methods, which were already successfully combined with recurrent neural networks to tackle variable sequential problems, see for instance \cite{naesseth2017variational,maddison2017filtering,Ma2020}.
We turn to \cite{Martin2020TheMC} for an example using more complex neural architectures, such as Transformer.
Particle filters have also proven reliable in other fields, such as presented in \cite{Liu2020LSTMPF} for object tracking.



%The former can be fitted through a traditional gradient descent iteration, while the latter is trained using Sequential Monte Carlo methods.
%Our uncertainty estimation layer can thus be built on top of an existing, already trained model. The model is presented in Section~\ref{sec:decoupling} and numerical experiments are displayed in Section~\ref{sec:exp}.

\section{Last layer decoupling}
Estimating the parameters of potentially high-dimensional models with unobserved (i.e. noisy) layers is a challenging task.
We therefore propose to first train an input model following traditional deep learning approaches, then use Monte Carlo methods in a lower dimensional state space to account for uncertainty, with tractable and computationally efficient simulation-based methods.
The two-stage training algorithm is presented in Algorithm~\ref{alg:particle_filter}, and the architecture of the model is described in Figure~\ref{fig:architecture}

In the following, for any sequence $(a_m,\ldots, a_n)$ with $n\geq m$, we use the short-hand notation $a_{m:n} = (a_m,\ldots, a_n)$.
Let $T\ge 1$ be a given time horizon.
We consider a regression task with observations $Y_{1:T}$ associated with inputs $U_{1:T}$.

\subsection{Representation learning}%
In this letter, we consider an arbitrary multi-layer neural network $h_\varphi$ with unknown parameters $\varphi$, responsible for extracting high level features from the input time series:
\begin{align*}
	\widetilde U_{1:T} & = h_\varphi(U_{1:T})\,, & \text{input model.}
\end{align*}
In order to get an estimate $\hat \varphi$, we also introduce an auxiliary function $\kappa_\psi$ to design a first recurrent architecture to model the observations: for all $1 \leq t \leq T$, $Y_t = \kappa_\psi(Y_{t-1}, \widetilde U_t)$ and $Y_0 = \kappa_\psi(\widetilde U_0)$.
The input model can then be trained by performing gradient descent to minimize the prediction error using a standard loss function.
We keep the estimated parameters $\hat \varphi$ while the auxiliary function $\kappa_\psi$, only designed to model the observations, is discarded.

\subsection{State space model}
\label{sub:proposed_architecture}
The next step is to define  a state space model taking as input the previously extracted features $\widetilde U_{1:T}$.
Let $X_{1:T}$ a sequence of stochastic hidden states computed recursively and $Y_t$ their associated predictions.
For all $t \geq 1$, the model is defined as:
\begin{align*}
	X_t & = g_\theta(X_{t-1}, \widetilde U_t) + \eta_t\,, & \text{state model, }       \\
	Y_t & = f_\theta(X_t) + \epsilon_t\,,                 & \text{observation model, }
\end{align*}
where $\theta$ are the unknown real-valued parameters of the network (weights and biases) and $f_\theta$ and $g_\theta$ are nonlinear parametric functions.
We chose $(\eta_t)_{t\geq 1}$ and $(\epsilon_t)_{t\geq 1}$ as two independent sequences of i.i.d. centered Gaussian random variables with covariance matrices $\Sigma_x$ and $\Sigma_y$, although any distribution can be substituted.

This decoupled approach aims at reducing the number of parameters in $\theta$, compared to $\varphi$, in order to estimate them using Sequential Monte Carlo methods.
In the next section, we describe this second training procedure for the last layer only, by keeping $\hat \varphi$ fixed.
All implementation and training details are postponed to Section~\ref{sec:exp}.

\begin{figure}[htpb]
	\centering
	\caption{Our architecture combining a generic input model with a state space model on the last layer.}
	\includegraphics[width=0.7\linewidth]{architecture.png}
	\label{fig:architecture}
\end{figure}

\section{Sequential Monte Carlo Layer}%
\label{sub:uncertainty_estimation}
In this section, we detail how to estimate the parameters $\theta$, $\Sigma_x$ and $\Sigma_y$ in the model introduced in Section~\ref{sub:proposed_architecture}, from a record of observations $Y_{1:T}$.
This is challenging because the loglikelihood of the observations is not available explicitly, as it would require integrating over the distribution of the hidden states $X_{1:T}$.
Consequently, the score function is intractable.
We propose to optimize a Monte Carlo estimator of this score function, using Fisher's identity \cite{monographie-randal}:
\begin{equation}
	\nabla_\theta \log p_\theta(Y_{1:T}) = \mathbb{E}_\theta \left[ \nabla_\theta\log p_\theta(X_{1:T}, Y_{1:T}) | Y_{1:T} \right]\,,
	\label{eq:grad_ll}
\end{equation}
where $\mathbb{E}_\theta$ designs the expectation under the model parameterized by $\theta$ (the dependency on the input $U_{1:T}$ is kept implicit here for better clarity).
In the following paragraphs, we denote by $\Psi_{\mu, \Sigma}$ the Gaussian probability density function with mean vector $\mu$ and covariance matrix $\Sigma$.

\subsection{Particle filter}
The conditional distribution of $X_{1:T}$ given $Y_{1:T}$ is not available explicitly for a nonlinear state space model, but it can be approximated using a family of particles $(\xi^{\ell}_{1:T})_{\ell=1}^N$ associated with importance weights $(\omega^{\ell}_T)_{\ell=1}^N$.
% We first produce a set of histories  associated with importance weights  using the Path-space smoother.
At $k = 0$, $(\xi^{\ell}_0)_{\ell=1}^N$ are sampled independently from $\rho_0 = \Psi_{0, \Sigma_x}$, and each particle $\xi^{\ell}_0$ is associated with the standard importance sampling weight:
\[
	\omega_0^{\ell} \propto  \Psi_{Y_0, \Sigma_y}(f_\theta(\xi^{\ell}_0)) \,.
\]
%where $\ell_0(\cdot) = \Psi_{Y_0, \Sigma_y}(f_\theta(\cdot))$.
Then, for $k\geq 1$, using $\{(\xi^{\ell}_{k-1},\omega^{\ell}_{k-1})\}_{\ell=1}^N$, we sample pairs $\{(I^{\ell}_k,\xi^{\ell}_{k})\}_{\ell=1}^N$ of indices and particles from the instrumental distribution:
\[
	\pi_{k}(\ell,x) \propto \omega_{k-1}^{\ell} p_k(\xi^{\ell}_{k-1},\widetilde U_k,x)\,.
\]
In this application we use for $p_k(\xi^{\ell}_{k-1},\widetilde U_k,\cdot)$ the prior kernel $\Psi_{g_\theta(\xi^\ell_{k-1}, \tilde U_k), \Sigma_x}$.
For $\ell \in \{1,\ldots,N\}$, $\xi^{\ell}_k$ is associated with the importance weight $\omega^{\ell}_k \propto \Psi_{Y_k, \Sigma_y}(f_\theta(\xi^{\ell}_k))$.%, where $\ell_k(\cdot) = \Psi_{Y_k, \Sigma_y}(f_\theta(\cdot))$.

\subsection{Particle smoother}
Any particle smoother can be used to estimate \eqref{eq:grad_ll} such as the Path-space smoother \cite{Kitagawa1996}, the Forward Filtering Backward Smoothing \cite{Doucet2000OnSM} or the Forward Filtering Backward Simulation algorithm \cite{Godsill2004MonteCS}.
Additionally, because estimating \eqref{eq:grad_ll} amounts to computing a smoothed expectation of an additive functional, we can also use very efficient forward-only SMC smoothers such as the PaRIS algorithm and its pseudo-marginal extensions \cite{Olsson2014EfficientPO,gloaguen2022pseudo}.
In order to illustrate our approach with the simplest (and computationally cheapest) SMC smoother, we opt for the Path-space smoother.
With $\xi^i_{1:T}$ the ancestral line of $\xi^i_{T}$, the score function \eqref{eq:grad_ll} can be estimated as follows using automated differentiation:
$$
	\widehat {S}^N_\theta(Y_{1:T}) = \sum_{\ell=1}^N \omega_T^\ell\nabla_\theta\log p_\theta(\xi^\ell_{1:T}, Y_{1:T})\,,
$$
where $p_\theta$ is the joint probability density function of $(X_{1:T}, Y_{1:T})$ for the model described in Section~\ref{sub:proposed_architecture}.


\subsection{Stochastic Gradient Descent for online estimation}
An appealing application of the last layer approach is recursive maximum likelihood estimation, i.e., where new observations are used only once to update the estimator of the unknown parameter $\theta$. In \cite{Brosse2020OnLA}, the authors used in particular Stochastic Gradient Descent (SGD) and Stochastic Gradient Langevin Dynamics to update the estimation of $\theta$ and perform uncertainty quantification.

In state space models, recursive maximum likelihood estimation produces a sequence $\lbrace\theta_k\rbrace_{k\geq 0}$ of parameter estimates writing, for each new observation $Y_{k},~k\geq 1$,
$$
	\theta_{k} = \theta_{k-1} + \gamma_k \nabla_\theta \ell_{\theta}(Y_k | Y_{0:k - 1}) \,,
$$
where $\ell_{\theta}(Y_k | Y_{0:k - 1})$ is the loglikelihood for the new observation given all the past, and $\lbrace\gamma_k\rbrace_{k\geq 1}$ are positive step sizes such that $\sum_{k \geq 1}\gamma_k = \infty$ and $\sum_{k \geq 1}\gamma_k^2 < \infty$. The practical implementation of such an algorithm, where $\nabla_\theta\ell_{\theta}(Y_k | Y_{0:k - 1})$ is approximated using the weighted samples $\{(\xi^{\ell}_k,\omega^{\ell}_k)\}_{\ell=1}^N$ can be found for instance in \cite{gloaguen2022pseudo}. The PaRIS algorithm proposed in \cite{Olsson2014EfficientPO} allows to use the weighted samples $\{(\xi^{\ell}_k,\omega^{\ell}_k)\}_{\ell=1}^N$ and some statistics $\{\tau^{\ell}_k\}_{\ell=1}^N$ on-the-fly to approximate $\nabla_\theta \ell_{\theta}(Y_k | Y_{0:k - 1})$ either using rejection sampling approaches or importance sampling steps.

Although this algorithm is very efficient to update parameters recusrively, it is computationally intensive and therefore fits particularly well our last layer approach as it would be intractable for very high dimensional latent states.

\begin{algorithm}
	\caption{Two-stage learning}
	\label{alg:particle_filter}
	\KwIn{$Y_{1:T}, U_{1:T}$}
	\KwOut{$(\hat \varphi, \hat \theta)$}
	$\hat \varphi \gets$ Train the input model $h_\varphi$\;
	$\widetilde U_{1:T} \gets h_{\hat \varphi}(U_{1:T})$\;
	Initialize parameter estimate $\widehat \theta_0$\;
	\For{$p \gets 1$ \KwTo $\mathrm{MaxIt}$}{
	$\xi_0^\ell \sim \rho_0$ and $\omega_0^{\ell} \propto  \Psi_{Y_0, \Sigma_y}(f_{\widehat \theta_{p-1}}(\xi^{\ell}_0))$\;
	\For{$k \gets 1$ \KwTo $T$}{
	\For{$j \gets 1$ \KwTo $N$}{
		$I_k^j \sim \mathbb{P}(I_k^j=m) = \omega_{k-1}^m$\;
		$\xi_k^j \sim p_k(\xi_{k-1}^{I_k^j},  \widetilde U_k,\cdot)$\;
		$\omega_k^j \propto  \Psi_{Y_k, \Sigma_y}(f_{\widehat \theta_{p-1}}(\xi^{j}_k))$\;
		Set $\xi_{0:k}^j = (\xi_{0:k-1}^{I_k^j},\xi_k^j)$.
		}
	}
	Update the parameter estimate using gradient descent with estimated gradient $\widehat {S}^N_{\widehat \theta_{p-1}}(Y_{1:T})$.
	}

\end{algorithm}

\section{Experiments}
\label{sec:exp}
\subsection{Data}
\label{sub:data}
We benchmarked our approach on the public Electricity Transformer Temperature (ETT) Dataset, designed in \cite{Zhou2021Informer} to forecast Oil temperature based on hourly power load records (ETTh1 subset).

\subsection{Model}%
\label{sub:models}

\textbf{The Input model} is a $L=3$ layered GRU model, as defined in the deep learning framework PyTorch\footnote{\href{https://pytorch.org/docs/stable/generated/torch.nn.GRU.html}{https://pytorch.org/docs/stable/generated/torch.nn.GRU.html}}: for all $1 \leq \ell \leq L$ and all $1 \leq t \leq T$,
\begin{align*}
	r^\ell_t        & = \sigma(W_{ir} U^{\ell - 1}_t + b_{ir} + W_{hr} U^{\ell}_{t-1} + b_{hr}) \,,                \\
	z^\ell_t        & = \sigma(W_{iz} U^{\ell - 1}_t + b_{iz} + W_{hz} U^{\ell}_{t-1} + b_{hz}) \,,                \\
	n^\ell_t        & = \mathrm{tanh}(W_{in} U^{\ell - 1}_t + b_{in} + r^\ell_t (W_{hn} U^\ell_{t-1} + b_{hn}))\,, \\
	\tilde U^\ell_t & = (1-z^\ell_t) n^\ell_t+z^\ell_t U^\ell_{t-1}\,,
\end{align*}
where $\varphi = \{(W_{is}, b_{is}, W_{hs}, b_{hs}), s \in \{r, z, n\}\}$ are unknown parameters, and $\sigma: x \mapsto 1/(1+\mathrm{e}^{-x})$ is the sigmoid function.
The first layer of the network is assimilated to the input vectors, $\widetilde U_t^0 \equiv U_t$ and $\widetilde U^\ell_0 \equiv 0$.
% where $\varphi = \{W_{ir}, b_{ir}, W_{hr}, b_{hr}, W_{iz}, b_{iz}, W_{hz}, b_{hz}, W_{in}, b_{in}, W_{hn}, b_{hn}\}$ are unknown parameters
The input dimension $d_\textnormal{in}=6$ corresponds to the number of power load records of the dataset, we set the output dimension to 6 as well.
In order to estimate the parameters $\varphi$, we introduce an auxiliary GRU layer responsible for computing oil temperature predictions.
During the training, me minimize the cost function $\mathcal{L}_{\mathrm{input}}(\varphi) = \sum_{i=1}^{N_{\texttt{sample}}} \|\texttt{model}_{\varphi}(U^i_{1:T}) - Y^i_{1:T}\|^2$ where for each sample $1 \leq i \leq N_{\texttt{sample}}$ in the dataset, $\texttt{model}_\varphi(U^i_{1:T})$ is the prediction associated with $Y^i_{1:T}$ obtained with this deterministic model.

\textbf{The State Space model} is implemented using PyTorch implementations of  RNN and Linear layers, in order to use auto differentiation.
We chose the following form for $f_\theta$ and $g_\theta$:
\begin{align*}
	g_\theta & : X_{t-1}, \widetilde U_t \mapsto \tanh(W_{gx} X_{t-1} + b_{gx} + W_{gu} \widetilde U_t + b_{gu})\,, \\
	f_\theta & : X_t \mapsto  \sigma(W_f X_t + b_f)\,,
\end{align*}
where $\theta = \{W_{gx}, b_{gx}, W_{gu}, b_{gu}, W_f, b_f\}$ are unknown parameters.
In this form, since the update rule for both $\Sigma_x$ and $\Sigma_y$ are available explicitly, we compute the following at each training step instead of using gradient descent:
\begin{align*}
	\Sigma_x & = T^{-1} \sum_{\ell=1}^N \sum_{t=1}^T \omega_T^\ell (\tanh^{-1}(\xi_t^\ell) - g_{\hat \theta}(\xi^\ell_{t-1}, \widetilde U_t))^2\,, \\
	\Sigma_y & = T^{-1} \sum_{\ell=1}^N \sum_{t=1}^T \omega_T^\ell (Y_t - f_{\hat \theta}(\xi^\ell_t))^2 \,.
\end{align*}
All following experiments were computed with $N=100$ particles.

%\subsection{Trainings}
%\label{sub:trainings}
All training experiments are conducted with a batch size of 32, using the Adam optimizer introduced in \cite{Kingma2015AdamAM}.
The learning rate was chosen using a simple grid search.
We train models for a maximum of 50 epochs, and employ early stopping to prevent overfit.

\subsection{Evaluations}%
\label{sub:evaluations}

In this section, we illustrate the ability of our model to capture the distribution of future observations, by evaluating the benchmarked models using the following protocol. We draw 48 hours long samples $(u_{1:48}, y_{1:48})$ from the validation dataset, composed of a 24 hour long lookback window $(u_{1:24}, y_{1:24})$, containing historic commands and observations, and a predictions window where only future commands are available $(u_{25:48})$.
Each model produces a 24 hour long forecast, which is compared to the ground truth to compute the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) criteria reported in Table~\ref{tab:ci_comparison}. % $\mathcal{L}(y_{25:48}, \texttt{model}(u_{1:48}, y_{1:24}))$,

For our proposed model, predictions can be performed by approximating the predictive density $p_{\theta,\varphi}(y_{t+1}|U_{1:t+1},Y_{1:t})$ by
$$
	p^N_{\widehat\theta,\widehat\varphi}(y_{t+1})= \sum_{i=1}^{N}\omega_t^i p_{\widehat\theta,\widehat\varphi}(y_{t+1}|\xi_t^i,U_{t+1})\,,
$$
where $ p_{\widehat\theta,\widehat\varphi}(y_{t+1}|\xi_t^i,U_{t+1})$ is the predictive distribution of $Y_{t+1}$ described in Section~\ref{sub:proposed_architecture}.
Although predictions given the previous time step provide good performances, it is limited as many applications require multi-steps forecasts.
In order to explore longer ranges, we can simply run our model to get $N$ samples for any time horizon $p$.
We report RMSE and MAE criteria by averaging the forecasts of these $N$ draws.
The associated intervals containing 95\% of the samples are displayed in Figure~\ref{fig:filter_k+24}, for $1\leq p \leq 24$.

We compared our model with MC Dropout methods, by implementing recurrent dropout layers as described in \cite{Gal2016NIPS}, with dropout values of $p_\textnormal{drop}=0.05$ and $p_\textnormal{drop}=0.01$.
The training procedure is similar to traditional recurrent models ; during inference, we draw $100$ samples from the dropout layers, and compute the same average forecasts and  intervals as for our model.
Despite being based on the same deep learning architecture, the MC dropout model is still largely overconfident, while our proposed model provide more credible empirical confidence intervals.
We also experimented with a Gaussian linear Hidden Markov Model (HMM) whose parameters are estimated with the Kalman smoother using the Expectation Maximization (EM, \cite{Dempster77EM}) algorithm.
Out of a range of possible latent dimension sizes $d_{\texttt{hmm}} \in \{1, 2, 4, 6\}$, we selected $d_{\texttt{hmm}}=4$ as it yielded the best performances.

\begin{figure}[htpb]
	\centering
	\caption{Prediction of Oil temperature (ETT Dataset) given observations in the lookback window ($t<24$) and without ($t>24$).
		Since re sampling of particles is no longer available at that point, the uncertainty grows for our model.
		As a comparison, we plotted the confidence intervals produced by the MC Dropout model (for $p_\textnormal{drop}=0.01$).
		Aggregated results on the entire validation set for the RMSE and MAE criteria can be found in Table~\ref{tab:ci_comparison}.}
	\includegraphics[width=\linewidth]{filter_kp24_ett.png}
	\includegraphics[width=\linewidth]{filter_kp24_ett_2.png}
	\label{fig:filter_k+24}
\end{figure}

\begin{table}[htpb]
	\centering
	\caption{Comparison of RMSE, MAE and computation time of our model against the benchmarked MC Dropout methods and HMM.
		This table provide aggregated results of the simulation presented in Figure~\ref{fig:filter_k+24} on the entire validation set.
		Two versions of the dropout model were evaluated, with dropout values $p_\textnormal{drop}=0.05$ and $p_\textnormal{drop}=0.01$.
		Mean values of the estimators are displayed along with their variance.}
	\label{tab:ci_comparison}
	\begin{tabular}{llll}
		\toprule
		             & RMSE            & MAE             & Computation time  \\
		\toprule
		SMCL (ours)  & $0.24 \pm 0.13$ & $0.21 \pm 0.12$ & $210 ms \pm 75.7$ \\
		MCD $p=0.01$ & $0.25 \pm 0.15$ & $0.21 \pm 0.14$ & $193 ms \pm 60.4$ \\
		MCD $p=0.05$ & $0.28 \pm 0.15$ & $0.24 \pm 0.13$ & $193 ms \pm 60.4$ \\
		HMM          & $0.46 \pm 0.19$ & $0.40 \pm 0.17$ & $994 ms \pm 21.4$ \\
		\bottomrule
	\end{tabular}
\end{table}

%Evaluations listed in priority order
%\begin{enumerate}
%	\item Compare confidence interval with MC-dropout model (soit LSTM dropout classique, soit en enlevant la derniere couche)
%	\item Compare with ARIMA/classic stat model
%	\item Compare MSE (average on particles) with classic finetuning: sample particles for half a week. For the second half, observations are not available at all ; thus we either take the mean of the predictions associated with each particles, or sample from the observation model and take the mean.
%	\item Compare linear SMC with kalman filter, sample under the gaussian law, plot boxplot
%\end{enumerate}

\section{Conclusion}%
\label{sec:conclusion}

In this letter, we introduced a decoupled architecture for uncertainty estimation on a time series dataset.
Our deep neural network backbone is responsible for extracting high level features, while particle filtering allows modelling recurrent non linear uncertainty.
Our proposed model improves confidence interval quality, compared to MC Dropout methods, while remaining simple and efficient to train, compared to VI alternatives.

We demonstrate the potential behind implementing latent space models as a modified RNN cell ;
more complex architectures, such as the GRU network used in the input model, or LSTM cells, could be considered.
Likewise, the Path-space smoother could be replaced by more performant alternatives.

Our decoupled architecture enables incorporating uncertainty estimation to an already trained network.
This opens the door to multiple, cheap finetuning of last layers parameters, from a global pretraining.
%For our application, this could translate in a generic training of the input model on an entire year of weather and building records, followed by season specific finetunings.

\clearpage
\bibliographystyle{IEEEtran}
\bibliography{refs.bib}
\end{document}

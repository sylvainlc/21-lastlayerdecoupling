% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}

\title{Last layer state space model}
%
% Single address.
% ---------------
\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
\address{Author Affiliation(s)}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%

\begin{document}
\maketitle

\begin{abstract}
	As sequential neural architectures become deeper and more complex, estimating the uncertainty of their predictions is ever so challenging.
	Efforts in quantifying uncertainty often rely on specific training procedures, and bear additional computational costs due to the dimensionality of such models.
	In this paper, we demonstrate how uncertainty can be estimated on top of an existing and trained neural network, combining a state space-based last  layer and  Sequential Monte Carlo methods. This approach allows to separate representation learning and uncertainty quantification. We apply our proposed methodology for the estimation of air quality in office buildings, through the hourly prediction of \ensuremath{\mathrm{CO_2}} levels.
	Our model accounts for the noisy data structure, due to unknown or unavailable variables (occupancy of the building, manual ventilation, etc.), and is able to provide confidence intervals on \ensuremath{\mathrm{CO_2}} predictions.
	We believe a deeper understanding of \ensuremath{\mathrm{CO_2}} variations can assist in regulating and reducing HVAC consumption, while improving well being.
\end{abstract}

\begin{keywords}
	One, two, three, four, five
\end{keywords}

\section{Introduction}
\label{sec:intro}

Recurrent Neural Networks (RNN) were first introduced as an efficient and convenient architecture to address short time dependencies problems \cite{Mozer1989AFB}.
They have been consistently improved to develop longer term memory, and optimize their implementations \cite{Bengio1994LearningLD,Hochreiter1997LongSM,Cho2014LearningPR}.
Current deep learning framework allows stacking arbitrary high number of recurrent layers, whose parameters are estimated by gradient descent through an automated differentiation procedure.

These recurrent architectures are able to approximate complex nonlinear time series, but suffer from overconfidence when evaluated outside of their training observations. Bayesian statistics aim at mitigating this drawback by providing uncertainty estimation \cite{Hinton1995BayesianLF,MacKay1992}.

Several architectures inspired by variational inference emerged by considering latent states as random variables and approximating the posterior.
The authors of \cite{Chung2015NIPS} built on a traditional RNN architecture by modelling temporal dependencies between these latent random states.
Results presented in \cite{Fortunato2017bayesian} yield improved performances when considering local gradient information for computing the posterior.
Similarly, introducing random variables directly into a neural network's weights can mitigate overconfidence, as demonstrated by the authors of \cite{Hinton1993}.
In \cite{Blundell2015}, weights are considered as distributions instead of vector values, allowing the network to make more reasonable predictions about unseen data.

Variational methods display encouraging results for modelling uncertainty in existing models, but require significant alteration of the model, as well as it's training mechanism.
In contrast, Monte Carlo Dropout (MC Dropout) methods offer to capture uncertainty by adding a single non parametric Dropout layer during both training and evaluation tasks, producing variable predictions from a single trained model, see \cite{Gal2016}.
These results were later extended to recurrent architectures in \cite{Gal2016NIPS}.
In the following years, MC Dropout methods have been applied in many industrial fields, such as anomaly detection (\cite{Zhu2017DeepAC}), flight delay prediction (\cite{Vandal2018}) or molecular simulations (\cite{Wen2020UncertaintyQI}).
Alternatively, ensemble methods consists in training distinct networks to obtain a combined prediction, as shown in \cite{Pearce2018}.
However, these frequentist approaches fail to guarantee proper calibration of the model, as highlighted by \cite{ashukha2020pitfalls}, and suffer various limitations, see \cite{Fong2020}.

In this paper, we introduce a new method for uncertainty estimation combining high expressivity, quality uncertainty estimations and ease of training.
Our proposed architecture is composed of an arbitrary sequential model, followed by a decoupled state space model layer.
The former can be fitted through a traditional gradient descent iteration, while the latter is trained using Sequential Monte Carlo methods.
Our uncertainty estimation layer can thus be built on top of an existing, already trained model.

\section{Background}
\label{sec:background}

\subsection{Recurrent neural networks}
\label{sec:background:rnn}
In this model, we implement a variation on the RNN, whose hidden states are computed as:
$$
	x_t = \tanh(W_x x_{t-1} + W_u u_t + b), \forall 1 \leq t \leq T
$$
where $u_t$ are the input at time $t$ and $\{W_u, W_x, b\}$ are the parameters of the model. The initial hidden value is often set to zero, $x_0 \equiv 0$.

This layer is meant to be plugged on top of an existing sequential architecture, whose outputted hidden states act as the input vector $u_t$.

\subsection{Sequential Monte Carlo methods}
\label{sec:background:smc}

We account for uncertainty in the pretrained architecture by modelling hidden states as random variables, and estimating their parameters.
Consequently, we can no longer integrate over their possible values, and computation of the gradient becomes intractable.
Instead, we leverage fisher's identity for an expression involving the posterior conditional expectation.
$$
\nabla \log p_\theta(y) = \mathbb{E}_\theta \left[ \nabla\log p_\theta(x, y) | Y \right]
$$

Sequential Monte Carlo methods aim at approximating this posterior by a set of weighted random samples.
At each time step, these particles are associated likelihood weights, given the previous observation, in order to perpetuate only the most likely candidates.
For any function $h$, we can approximate the conditional expectation as follows:
\begin{align*}
	\mathbb{E}_\theta \left[ h(x) | y \right] = \sum_{i=1}^N \omega_T^i h(\xi^i)
\end{align*}

\section{Last layer decoupling}
\label{sec:decoupling}
Our proposed architecture can be decomposed in two decoupled blocks.
An input model is responsible for extracting relevant features from the input time series, and can assume any architecture.
Its final hidden states are fed to a Sequential Monte Carlo Layer (SMCL), based on a plain RNN, for uncertainty estimation.

Considering an input model $f_\varphi$, our model is defined as follow:
\begin{equation*}
	\left\{
	\begin{aligned}
		\tilde u_h & = f_\varphi(u_t)                                     \\
		h_t        & = f_\theta(h_{t-1}, \tilde u_t, \eta_t) \\
		y_t        & = W_y h_t + b_y + \epsilon_t                         \\
	\end{aligned}
	\right.
\end{equation*}

Where $\eta$ and $\epsilon$ are two sequences of i.i.d. real valued noises with covariance matrices $\Sigma_x$ and $\Sigma_y$.

Provide simple equations for the smc model, along with formulas for the $\Sigma_{x, y}$ updates.

Protocol:
\begin{enumerate}
	\item Training with classic methods (train dataset)
	\item Finetuning with MC (train dataset)
\end{enumerate}

\begin{itemize}
	\item Un par introduction state model
	\item par description methodes monte carlo --> background ?
	\item precision pour notre algo
\end{itemize}

\section{Experiments}
\label{sec:exp}

\subsection{Training}%
\label{sub:training}

\begin{itemize}
	\item Moins important ; pas besoin de courbes
	\item Compare EM and gradient training for $\Sigma_{x, y}$ estimation
\end{itemize}

\subsection{Visualizations}%
\label{sub:visualizations}

Visualizations listed in priority order
\begin{enumerate}
	\item Prediction at t+1: plot t+1 predictions as boxplots, along with the mean and the observations.
	\item Prediction at t+k
	\item Compare weights evolution with classic finetuning
	\item Smoothed predictions: Smoothed particles trajectories' associated prediction: sample particles from the posterior, smooth the trajectories, apply the model function, and plot the results as a interval.
\end{enumerate}

\subsection{Evaluations}%
\label{sub:evaluations}

Evaluations listed in priority order
\begin{enumerate}
	\item Compare confidence interval with MC-dropout model (soit LSTM dropout classique, soit en enlevant la derniere couche)
	\item Compare with ARIMA/classic stat model
	\item Compare MSE (average on particles) with classic finetuning: sample particles for half a week. For the second half, observations are not available at all ; thus we either take the mean of the predictions associated with each particles, or sample from the observation model and take the mean.
	\item Compare linear SMC with kalman filter, sample under the gaussian law, plot boxplot
\end{enumerate}


% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
%\begin{figure}[htb]

%\begin{minipage}[b]{1.0\linewidth}
%  \centering
%  \centerline{\includegraphics[width=8.5cm]{image1}}
%%  \vspace{2.0cm}
%  \centerline{(a) Result 1}\medskip
%\end{minipage}
%%
%\begin{minipage}[b]{.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image3}}
%%  \vspace{1.5cm}
%  \centerline{(b) Results 3}\medskip
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image4}}
%%  \vspace{1.5cm}
%  \centerline{(c) Result 4}\medskip
%\end{minipage}
%%
%\caption{Example of placing a figure with experimental results.}
%\label{fig:res}
%%
%\end{figure}


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs.bib}

\end{document}

\documentclass[journal]{IEEEtran}

% Packages
\usepackage{amsmath} % Math environments
\usepackage{amsfonts, dsfont} % Math fonts
\usepackage{hyperref} % Links
\usepackage{graphicx} % Include graphics
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\graphicspath{ {./images/} }

\title{Last layer state space model for representation learning and uncertainty quantification}

\author{Max Cohen, Maurice Charbit and Sylvain Le Corff
	\thanks{This work was supported by grants from Région Ile-de-France.}
	\thanks{Max Cohen and Sylvain Le Corff are with Samovar, T\'el\'ecom SudParis, CITI, TIPIC, Institut Polyechnique de Paris, (e-mail: \{max.cohen, sylvain.le\_corff\}@telecom-sudparis.eu).}
	\thanks{Maurice Charbit is with Accenta, Boulogne-Billancourt (e-mail: mch@oze-energies.com).}}

\begin{document}
\maketitle
\begin{abstract}
	As sequential neural architectures become deeper and more complex, uncertainty estimation is more and more challenging.
	Efforts in quantifying uncertainty often rely on specific training procedures, and bear additional computational costs due to the dimensionality of such models.
	In this letter, we propose to decompose a classification or regression task in two steps: a representation learning stage to learn low-dimensional states, and a state space model for uncertainty estimation.
	This approach allows to separate representation learning and design of generative models.
	We demonstrate how predictive distributions can be estimated on top of an existing and trained neural network, by adding a state space-based last layer whose parameters are estimated with Sequential Monte Carlo methods.
	We apply our proposed methodology to the hourly estimation of Electricity Transformer Oil temperature, a publicly benchmarked dataset.
	Our model accounts for the noisy data structure, due to unknown or unavailable variables, and is able to provide confidence intervals on predictions.
\end{abstract}

\begin{IEEEkeywords}
	Recurrent neural networks, Representation learning, Uncertainty quantification, Sequential Monte Carlo.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
% We believe a deeper understanding of \ensuremath{\mathrm{CO_2}} variations can assist in regulating and reducing HVAC consumption, while improving well being.

Recurrent Neural Networks (RNN) were first introduced as an efficient and convenient architecture to address short time dependencies problems.
They have been consistently improved to develop longer term memory, and optimize their implementations \cite{Bengio1994LearningLD,Hochreiter1997LongSM}. %,Cho2014LearningPR}.
Current deep learning frameworks allow stacking arbitrary high number of recurrent layers, whose parameters are estimated by gradient descent through automated differentiation procedures, as shown in \cite{Graves2013SpeechRecognition}.
However, many critical applications, such as medical diagnosis or drug design discovery, require not only accurate predictions, but a good estimate of their uncertainty (\cite{Crowson2016AssessingCalibration, Mervin2020UncertaintyQuantification}).
Fostering the dissemination of deep learning-based algorithms to such fields requires to design new approaches for uncertainty quantification.

In this context, Bayesian statistics enables approximating the distributions of future observations and to provide uncertainty estimation \cite{Hinton1995BayesianLF}.
Several architectures inspired by Variational Inference (VI, see \cite{Jordan2004AnIT}) emerged by considering latent states as random variables and approximating their posterior distribution.
The authors of \cite{Chung2015NIPS} built on a traditional RNN architecture by modelling temporal dependencies between these latent random states.
Results presented in \cite{Fortunato2017bayesian} yield improved performances when considering local gradient information for computing the posterior.
%Similarly, introducing random variables directly into a neural network weights can mitigate overconfidence, as demonstrated by the authors of \cite{Hinton1993}.
In \cite{Blundell2015}, the authors considered weights as random variables and proposed approximations of their posterior distributions allowing more robust predictions on unseen data.

%Variational methods display encouraging results for modelling uncertainty in existing models, but require significant alteration of the model, as well as its training mechanism.
Monte Carlo Dropout (MC Dropout) methods offer to capture uncertainty by leveraging Dropout during both training and evaluation tasks, producing variable predictions from a single trained recurrent model, see \cite{Gal2016NIPS}.
In the following years, MC Dropout methods have been applied in many industrial fields, such as flight delay prediction (\cite{Vandal2018}) or molecular simulations (\cite{Wen2020UncertaintyQI}).
Alternatively, ensemble methods consists in training distinct networks to obtain a combined prediction, as shown in \cite{Pearce2018}.
However, these frequentist approaches fail to guarantee proper calibration of the model, as highlighted by \cite{ashukha2020pitfalls}, and suffer various limitations, see \cite{Fong2020}.

In an effort to provide an alternative strategy with limited computation overhead, \cite{Brosse2020OnLA} suggests splitting training in two stages to solve classification problems for independent data: representation learning and uncertainty estimation. The two steps proceed as follows: (i) the algorithm first trains and end-to-end a deep classifier to obtain accurate task-dependent representations of the data, and then (ii) ensemble models are trained using these representations and the output. Their experiments indicate that last-layer algorithms outperform baseline networks and that a single last layer is an appealing trade-off between computational cost and uncertainty quantification. However, this method is restricted to independent and identically distributed data and cannot be directly applied to time series.


Inspired by  \cite{Brosse2020OnLA}, we propose a last layer approach to split uncertainty quantification from representation learning for dependent data.  This new method for uncertainty estimation combines high expressivity, quality uncertainty estimations and ease of training. Our proposed architecture is composed of an arbitrary sequential model, followed by a decoupled state space model layer. Using a state space model in the last layer allows to introduce complex predictive distributions for the observations based on task-dependent latent data. However, in such a setting, the loglikelihood of the observations is not available explicitly and approximate sampling methods are required for the second training stage.   In this letter, we explore Sequential Monte Carlo methods, which were already successfully combined with recurrent neural networks to tackle variable sequential problems, see for instance \cite{Ma2020}.
We turn to \cite{Martin2020TheMC} for an example using more complex neural architectures, such as Transformer.
Particle filters have also proven reliable in other fields, such as presented in \cite{Liu2020LSTMPF} for object tracking.



%The former can be fitted through a traditional gradient descent iteration, while the latter is trained using Sequential Monte Carlo methods.
%Our uncertainty estimation layer can thus be built on top of an existing, already trained model. The model is presented in Section~\ref{sec:decoupling} and numerical experiments are displayed in Section~\ref{sec:exp}.

\section{Last layer decoupling}
\label{sec:decoupling}

\subsection{Proposed model}%
\label{sub:proposed_architecture}

In the following, for any sequence $(a_m,\ldots, a_n)$ with $n\geq m$, we use the short-hand notation $a_{m:n} = (a_m,\ldots, a_n)$.
Let $T\ge 1$ be a given time horizon.
We consider the regression task over observations $Y_{1:T}$ associated with the inputs $U_{1:T}$.
Traditionaly, at each time $t$, a state $h_t$ is computed recursively: $h_t = g_\theta(h_{t-1},U_t)$ and $Y_t$ is predicted by a transform $f_\theta(h_t)$, where $\theta$ is a set of real-valued parameters of the networks (weights and bias) and $f_\theta$ and $g_\theta$ are parametric functions.
The parameters are usually trained using stochastic gradient based algorithms with a regularized mean squared error loss function.

In our approach, we introduce a sequence $X_{1:T}$ of stochastic hidden states in the last layer of the model.
An input model $h_\varphi$ is responsible for extracting high level features from the input time series. Then, the final hidden states of this network are fed to a state space layer.
For all $t \geq 1$, the model is defined as:
\begin{equation*}
	\left\{
	\begin{aligned}
		\widetilde U_t & = h_\varphi(U_t)                             & \text{input model, }       \\
		X_t            & = g_\theta(X_{t-1}, \widetilde U_t) + \eta_t & \text{state model, }       \\
		Y_t            & = f_\theta(X_t) + \epsilon_t                 & \text{observation model, } \\
	\end{aligned}
	\right.
\end{equation*}
We chose $(\eta_t)_{t\geq 1}$ and $(\epsilon_t)_{t\geq 1}$ as two sequences of i.i.d. centered Gaussian random variables with covariance matrices $\Sigma_x$ and $\Sigma_y$, although any distribution can be substituted.
$\varphi$ is the set of unknown parameters associated with the input model.
In this letter, we focus on settings where $h_\varphi$ is an arbitrary multi-layer neural network, and $(g_\theta, f_\theta)$ are non linear transformations.
Estimating the parameters of potentially high-dimensional models with unobserved (i.e. noisy) layers is a challenging task.
We therefore propose to first train the input model following classical deep learning approaches and then use Monte Carlo methods in a lower dimensional state space to account for uncertainty in the last layer.

\subsection{Input model}%
\label{sub:input_model}
In this letter, we chose a multi-layered GRU network as the input model, such that each hidden state is computed as follows, for all $1 \leq \ell \leq L$ and all $1 \leq t \leq T$,
$$
	\tilde U^\ell_t = k_\varphi^\ell(\widetilde U^{\ell-1}_t, \widetilde U^\ell_{t-1})\,,
$$
where the first layer is assimilated to the input vectors, $\widetilde U_t^0 \equiv U_t$, and $\widetilde U^\ell_0 \equiv 0$. The specific choice of functions $ k_\varphi^\ell$ are detailed in Section~\ref{sec:exp}.
The first step of the decoupling procedure proposed in this letter is to estimate all parameters by considering the entire model as a deterministic neural network, i.e. by ignoring the noises $(\eta_t)_{t\geq 1}$ and $(\epsilon_t)_{t\geq 1}$ in the model.
This step can be performed efficiently by minimizing a loss unction $\mathcal{L}_{\mathrm{input}}$ using stochastic gradient descent to obtain a first set of estimators $\widehat \varphi$ and $ \widehat \theta$.
The input model weights $\widehat \varphi$ are kept fixed in the following section, while the parameters $\widehat \theta$ is used as an initial estimate for the inference of the state space layer.
All details on the training of the input model are postponed to Section~\ref{sec:exp}.

\subsection{Sequential Monte Carlo Layer}%
\label{sub:uncertainty_estimation}

Estimating the parameter $\theta$, $\Sigma_x$ and $\Sigma_y$ in the model introduced in Section~\ref{sub:proposed_architecture} from a record of observations $Y_{1:T}$ is challenging as the loglikelihood of the observations is not available explicitly, as it requires an integration over the distribution of the hidden states $X_{1:T}$. Consequently, the computation of the score function is intractable.
In this letter we use Fisher's identity to propose a Monte Carlo estimator of the score function:
\begin{equation}
	\nabla_\theta \log p_\theta(Y_{1:T}) = \mathbb{E}_\theta \left[ \nabla_\theta\log p_\theta(X_{1:T}, Y_{1:T}) | Y_{1:T} \right]\,,
	\label{eq:grad_ll}
\end{equation}
where $\mathbb{E}_\theta$ design the expectation under the model of Section~\ref{sub:proposed_architecture} parameterized by $\widehat \varphi$ and $\theta$. The conditional distribution of $X_{1:T}$ given $Y_{1:T}$ is not available explicitly for the nonlinear state space model proposed in this letter. %A solution to approximate \eqref{eq:grad_ll} is to obtain expectedly low variance Monte Carlo estimates of the smoothed expectation.
%This is a challenging issue as more and more practical cases are modeled using high dimensional state spaces (which means high dimensional integrals in \eqref{eq:def:smoothing}) and produce huge data sets (which means that $n$ is large and implies great storage and computational costs). Online parameter inference, which computes new parameter estimates on-the-fly as observations are received, also implies additional computational constraints which rules out a vast majority of standard maximum likelihood procedures. This chapter highlights some recent results on the analysis of the approximation of smoothing distributions using Sequential Monte Carlo (SMC) methods and their applications to (online) parameter inference.
%Sequential Monte Carlo methods aim at approximating this posterior by a set of $N$ weighted random trajectories.
The auxiliary particle filter introduced in \cite{Jun1998} produces first recursively a set of states $(\xi^{\ell}_t)_{\ell=1}^N$ associated with importance weights $(\omega^{\ell}_t)_{\ell=1}^N$ using importance sampling and resampling steps.
At each time step $t$, the weighted samples $\{(\xi^{\ell}_t,\omega^{\ell}_t)\}_{\ell=1}^N$ approximate the filtering distribution at time $t$, i.e. the conditional distribution of $X_t$ given $Y_{0:t}$.
%At each time step $1\leq t \leq T$, these particles $\xi^{1:N}_t$ are associated likelihood weights $\omega^{1:N}_t$,
%The propagation and selection steps are recalled in Algorithm~\ref{algo:particle_filter}.
Based on this particle filter, any particle smoother can be used to estimate \eqref{eq:grad_ll} such as the Path-space smoother \cite{Kitagawa1996}, the Forward Filtering Backward Smoothing \cite{Doucet2000OnSM} or the Forward Filtering Backward Simulation algorithm \cite{Godsill2004MonteCS}.
Using the model provided in Section~\ref{sub:proposed_architecture}, estimating \eqref{eq:grad_ll} amounts to computing a smoothed expectation of an additive functional so that very efficient forward-only SMC smoothers can also be used such as the PaRIS algorithm and its pseudo-marginal extensions \cite{Olsson2014EfficientPO,gloaguen2022pseudo}.
%Such algorithms allow to approximate, for any function $h$, any smoothing expectation as follows:
%\begin{align*}
%	\mathbb{E}_\theta \left[ h(X_{1:T}) | Y_{1:T} \right] = \sum_{i=1}^N \omega_T^i h(\xi^i_{1:T})\,.
%\end{align*}
%The Poor man's smoother
Plugging such an estimate in \ref{eq:grad_ll} provides and explicit loss function to estimate $\theta$ which can be computed using deep learning frameworks through automated differentiation. %In this letter, we use a simple Sequential Monte Carlo smoother to illustrate the performance of the proposed methodology. %. \textcolor{red}{Dire un mot sur le fait que les SMC sont durs à calibrer en grande dimension, mais qu'ils sont justement appropriés dans ce contexte où seul un bloc contient des variables bruitées.}
%\begin{align*}
%	\mathbb{J}(\theta) & = \log |\Sigma_x| + \log |\Sigma_y| \\
%	 & + \frac{1}{T}\sum_{k=0}^T \sum_{i=1}^N \omega^i (y_k - f_\theta(\xi_k^i))' \Sigma_y^{-1} (y_k - f_\theta(\xi_k^i)) \\
%	 & + \frac{1}{T}\sum_{k=1}^T \sum_{i=1}^N \omega^i (\xi_k^i - g_\theta(\xi_{k-1}^i, u_k))'\Sigma_x^{-1}(\xi_k^i - g_\theta(\xi_{k-1}^i, u_k))
%\end{align*}
%During the estimation of $\theta$, the input model is frozen, i.e. we stop gradient computation for $\varphi$. Both $\Sigma_x$ and $\Sigma_y$ are updated by an explicit EM, as gradient descent for these parameters proved unstable.

% \begin{algorithm}
% 	\KwIn{$Y_{1:T}, \widetilde U_{1:T}$}
% 	\KwOut{$\xi_{1:T}^{1:N}$}
% 	$\xi_0^i \gets \eta^i$\;
% 	\For{$t \gets 0$ \KwTo $T$}{
% 		$\omega_t^i \gets \varphi_{y_t, \Sigma_y}(f_\theta(\xi_t^i))$\;
% 		$I_{t+1}^i \sim \mathbb{P}(I_{t+1}^i=j) = \omega_t^j$\;
% 		$\xi_{t+1}^i \gets g_\theta(\xi_t^{I_{t+1}^i}, \tilde u_{t+1}, \eta^i)$\;
% 	}
% 	\caption{Particle filter. $\varphi_{\mu, \Sigma}$ is the multivariate normal density function with mean $\mu$ and covariance matrix $\Sigma$.}
% 	\label{algo:particle_filter}
% \end{algorithm}

%\begin{algorithm}
%	\KwIn{Observations and inputs $(Y^k_{1:T},U^k_{1:T})_{k=1}^{N_{\texttt{sample}}}$. Initial estimates of $\varphi$, $\theta$.}
%	%\KwOut{$\xi_{1:T}^{1:N}$}
%	Estimate $(\widehat\varphi,\widehat\theta)$ by training the deterministic neural network with loss $\mathcal{L}_{\mathrm{inp}}$ and data $(Y^k_{1:T},U^k_{1:T})_{k=1}^{N_{\texttt{sample}}}$\;
%	Initialize $\widetilde \theta_0 = \widehat \theta$, $\Sigma_x$ and $\Sigma_y$\;
%	\For{each batch $1\leq k\leq N_{\texttt{sample}}$}{
%		Compute a SMC estimate $\widehat S_k^N$ of the score \eqref{eq:grad_ll}\;
%		Compute $\widetilde \theta_{k+1}$ and update $\Sigma_x$ and $\Sigma_y$ by stochastic gradient descent using $\widehat S_k^N$\;
%	}
%	%$\xi_0^i \gets \eta^i$\;
%	%\For{$t \gets 0$ \KwTo $T$}{
%	%	$\omega_t^i \gets \varphi_{y_t, \Sigma_y}(f_\theta(\xi_t^i))$\;
%	%	$I_{t+1}^i \sim \mathbb{P}(I_{t+1}^i=j) = \omega_t^j$\;
%	%	$\xi_{t+1}^i \gets g_\theta(\xi_t^{I_{t+1}^i}, \tilde u_{t+1}, \eta^i)$\;
%	%}
%	\caption{Estimation of the two blocks.}
%	\label{algo:allsteps}
%\end{algorithm}

\subsection{Stochastic Gradient Descent for online estimation}
An appealing application of the last layer approach is recursive maximum likelihood estimation, i.e., where new observations are used only once to update the estimator of the unknown parameter $\theta$. The algorithm produces a sequence $\lbrace\theta_k\rbrace_{k\geq 0}$ of parameter estimates writing, for each new observation $Y_{k},~k\geq 1$,
$$
	\theta_{k} = \theta_{k-1} + \gamma_k \nabla_\theta \ell_{\theta}(Y_k | Y_{0:k - 1}) \,,
$$
where $\ell_{\theta}(Y_k | Y_{0:k - 1})$ is the loglikelihood for the new observation given all the past, and $\lbrace\gamma_k\rbrace_{k\geq 1}$ are positive step sizes such that $\sum_{k \geq 1}\gamma_k = \infty$ and $\sum_{k \geq 1}\gamma_k^2 < \infty$. The practical implementation of such an algorithm, where $\ell_{\theta}(Y_k | Y_{0:k - 1})$ is approximated using the weighted samples $\{(\xi^{\ell}_k,\omega^{\ell}_k)\}_{\ell=1}^N$ can be found for instance in \cite{}. Although this algorithm is very efficient to update parameters recusrively, it is computationally intensive and therefore fits particularly well our last layer approach as it would be intractable for very high dimensional latent states.

\section{Experiments}
\label{sec:exp}

\subsection{Data}
\label{sub:data}
We benchmarked our approach on the public Electricity Transformer Temperature (ETT) Dataset, designed in \cite{Zhou2021Informer} around the forecasting of Oil temperature based on hourly power load records (ETTh1 subset).
We drew samples composed of a 24 hour long lookback window, where models have access to both commands and observations, followed by a 24 hour long forecasting period, where their performances are compared to the ground truth.

\subsection{Models}%
\label{sub:models}

\textbf{The Input model} is a $L=3$ layered GRU model, as defined in the deep learning framework PyTorch\footnote{\href{https://pytorch.org/docs/stable/generated/torch.nn.GRU.html}{https://pytorch.org/docs/stable/generated/torch.nn.GRU.html}}: for all $1 \leq \ell \leq L$ and all $1 \leq t \leq T$,
\begin{align*}
	r^\ell_t        & = \sigma(W_{ir} U^{\ell - 1}_t + b_{ir} + W_{hr} U^{\ell}_{t-1} + b_{hr})        \\
	z^\ell_t        & = \sigma(W_{iz} U^{\ell - 1}_t + b_{iz} + W_{hz} U^{\ell}_{t-1} + b_{hz})        \\
	n^\ell_t        & = tanh(W_{in} U^{\ell - 1}_t + b_{in} + r^\ell_t (W_{hn} U^\ell_{t-1} + b_{hn})) \\
	\tilde U^\ell_t & = (1-z^\ell_t) * n^\ell_t+z^\ell_t * U^\ell_{t-1}
\end{align*}
where $\varphi = \{(W_{is}, b_{is}, W_{hs}, b_{hs}), s \in \{r, z, n\}\}$ are unknown parameters.
% where $\varphi = \{W_{ir}, b_{ir}, W_{hr}, b_{hr}, W_{iz}, b_{iz}, W_{hz}, b_{hz}, W_{in}, b_{in}, W_{hn}, b_{hn}\}$ are unknown parameters
The input dimension $d_\textnormal{in}=6$ corresponds to the number of power load records of the dataset, we set the output dimension to 6 as well.
In order to estimate the parameters $\varphi$, we add an additional GRU layer responsible for computing oil temperature predictions.
During the training, me minimize the cost function $\mathcal{L}_{\mathrm{input}}(\varphi) = \sum_{i=1}^{N_{\texttt{sample}}} \|\texttt{model}_{\varphi}(U^i_{1:T}) - Y^i_{1:T}\|^2$ where for each sample $1 \leq i \leq N_{\texttt{sample}}$ in the dataset, $\texttt{model}_{\varphi,\theta}(U^i_{1:T})$ is the prediction associated with $Y^i_{1:T}$ obtained with this deterministic model.

\textbf{The Sequential Monte Carlo layer} uses the simple Path-space smoother where, writing $\xi_{1:T}^i$ the ancestral trajectory of $\xi_T^i$ for all $1\leq i \leq N$, we can approximate \eqref{eq:grad_ll} with:
$$
	\widehat {\nabla_\theta \log p_\theta(Y_{1:T})_k^N} = \sum_{i=1}^N \omega_T^i\nabla_\theta\log p_\theta(\xi^i_{1:T}, Y^{k}_{1:T})\,.
$$
During forecasting, we implemented a bootstrap filter where particles are sampled from the prior model and weighted with the observation model.
All following experiments were computed with $N=100$ particles.

\subsection{Trainings}
\label{sub:trainings}
All training experiments are conducted with a batch size of 32, using the Adam optimizer introduced in \cite{Kingma2015AdamAM}.
The learning rate was chosen using a simple grid search.
We train models for a maximum of 50 epochs, and employ early stopping to prevent overfit.

\subsection{Evaluations}%
\label{sub:evaluations}

In this section, we illustrate the ability of our multi-layer model to capture the distribution of future observations.
First, one step predictions can be performed by approximating the predictive density $p_{\theta,\varphi}(y_{t+1}|U_{1:t+1},Y_{1:t})$ by
$$
	p^N_{\widehat\theta,\widehat\varphi}(y_{t+1})= \sum_{i=1}^{N}\omega_t^i p_{\widehat\theta,\widehat\varphi}(y_{t+1}|\xi_t^i,U_{t+1})\,,
$$
where $ p_{\widehat\theta,\widehat\varphi}(y_{t+1}|\xi_t^i,U_{t+1})$ is the predictive distribution of $Y_{t+1}$ described in Section~\ref{sub:proposed_architecture}.
Although predictions given the previous time step provide good performances, it is limited as many applications require multi-steps forecasts.
In order to explore longer ranges, we can draw $N$ independent samples from $\sum_{i=1}^{N}\omega_t^i p_{\widehat\theta,\widehat\varphi}(y_{t+p}|\xi_t^i,U_{t+1:t+p})$, for $p>1$.
As $p$ increases, the accuracy of our predictions decreases, as our model no longer has access to the observations.
We report MSE and MAE criteria on the ETT Dataset, obtained by averaging the forecasts of these $N$ draw, in Table~\ref{tab:ci_comparison}.
The associated confidence intervals are displayed in Figure~\ref{fig:filter_k+24} for $1\leq p \leq 24$.

We benchmark our proposed architecture with MC Dropout methods, by implementing recurrent dropout layers as described in \cite{Gal2016NIPS}, with dropout values of $p_\textnormal{drop}=0.05$ and $p_\textnormal{drop}=0.01$.
The training procedure is similar to traditional recurrent models ; during inference, we draw $N=100$ samples from the dropout layers, and compute the same average forecasts and confidence intervals as our model.
Despite being based on the same deep learning architecture, the MC dropout model is still largely overconfident, while our proposed model provide more credible empirical confidence intervals.
We also experimented with a linear Hidden Markov Model (HMM), without reaching performances on par with the other approaches.

\begin{figure}[htpb]
	\centering
	\caption{Prediction of Oil temperature (ETT Dataset) given observations in the lookback window ($t<24$) and without ($t>24$).
		Since re sampling of particles is no longer available at that point, the uncertainty grows for our model.
		As a comparison, we plotted the confidence intervals produced by the MC Dropout model (for $p_\textnormal{drop}=0.01$).
		Aggregated results on the entire validation set for the MSE and MAE criteria can be found in Table~\ref{tab:ci_comparison}.}
	\includegraphics[width=\linewidth]{filter_kp24_ett.png}
	\label{fig:filter_k+24}
\end{figure}

\begin{table}[htpb]
	\centering
	\caption{Comparison of Mean Squared Error (MSE), Mean Absolute Error (MAE) and computation time of our model against the benchmarked MC Dropout methods and HMM.
		This table provide aggregated results of the simulation presented in Figure~\ref{fig:filter_k+24} on the entire validation set.
		Two versions of the dropout model were evaluated, with dropout values $p_\textnormal{drop}=0.05$ and $p_\textnormal{drop}=0.01$.
		Mean values of the estimators are displayed along with their variance.}
	\label{tab:ci_comparison}
	\begin{tabular}{llll}
		\toprule
		$(\times 10^{-2})$ & MSE             & MAE             & Computation time  \\
		\toprule
		SMCL (ours)        & $7.74 \pm 9.84$ & $21.2 \pm 12.1$ & $210 ms \pm 75.7$ \\
		MCD $p=0.01$       & $8.20 \pm 14.0$ & $20.9 \pm 14.0$ & $193 ms \pm 60.4$ \\
		MCD $p=0.05$       & $9.76 \pm 10.5$ & $23.4 \pm 13.4$ & $193 ms \pm 60.4$ \\
		HMM                & $76.2 \pm 28.1$ & $65.6 \pm 26.3$ & $994 ms \pm 21.4$ \\
		\bottomrule
	\end{tabular}
\end{table}

%Evaluations listed in priority order
%\begin{enumerate}
%	\item Compare confidence interval with MC-dropout model (soit LSTM dropout classique, soit en enlevant la derniere couche)
%	\item Compare with ARIMA/classic stat model
%	\item Compare MSE (average on particles) with classic finetuning: sample particles for half a week. For the second half, observations are not available at all ; thus we either take the mean of the predictions associated with each particles, or sample from the observation model and take the mean.
%	\item Compare linear SMC with kalman filter, sample under the gaussian law, plot boxplot
%\end{enumerate}

\section{Conclusion}%
\label{sec:conclusion}

In this letter, we introduced a decoupled architecture for uncertainty estimation on a time series dataset.
Our deep neural network backbone is responsible for extracting high level features, while particle filtering allows modelling recurrent non linear uncertainty.
Our proposed model improves confidence interval quality, compared to MC Dropout methods, while remaining simple and efficient to train, compared to VI alternatives.

We demonstrate the potential behind implementing latent space models as a modified RNN cell ;
more complex architectures, such as the GRU network used in the input model, or LSTM cells, are left for future works.
Likewise, the Path-space smoother could be replaced by faster and more performant alternatives described above.

Our decoupled architecture enables incorporating uncertainty estimation to an already trained network.
This opens the door to multiple, cheap finetuning of last layers parameters, from a global pretraining.
%For our application, this could translate in a generic training of the input model on an entire year of weather and building records, followed by season specific finetunings.

\clearpage
\bibliographystyle{IEEEtran}
\bibliography{refs.bib}
\end{document}

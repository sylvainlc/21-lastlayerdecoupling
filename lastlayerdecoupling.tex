% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}

\title{Last layer state space model}
%
% Single address.
% ---------------
\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
\address{Author Affiliation(s)}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%

\begin{document}
\maketitle

\begin{abstract}
	As sequential neural architectures become deeper and more complex, estimating the uncertainty of their predictions is ever so challenging.
	Efforts in quantifying uncertainty often rely on specific training procedures, and bear additional computational costs due to the dimensionality of such models.
	In this paper, we demonstrate how uncertainty can be estimated on top of an existing and trained neural network, combining a state space-based last  layer and  Sequential Monte Carlo methods. This approach allows to separate representation learning and uncertainty quantification. We apply our proposed methodology for the estimation of air quality in office buildings, through the hourly prediction of \ensuremath{\mathrm{CO_2}} levels.
	Our model accounts for the noisy data structure, due to unknown or unavailable variables (occupancy of the building, manual ventilation, etc.), and is able to provide confidence intervals on \ensuremath{\mathrm{CO_2}} predictions.
	We believe a deeper understanding of \ensuremath{\mathrm{CO_2}} variations can assist in regulating and reducing HVAC consumption, while improving well being.
\end{abstract}

\begin{keywords}
	One, two, three, four, five
\end{keywords}

\section{Introduction}
\label{sec:intro}

Bayesian statistics are often combined with the expressiveness of deep learning models to provide uncertainty estimation \cite{Hinton1995BayesianLF,MacKay1992}.


\begin{itemize}
	\item Describe the active research on Bayesian RNN and uncertainty for time series (cf Alice paper with transformer).
	\item Describe the aim: separating  representation learning and uncertainty estimation (cf Nicolas Brosse paper on last layer classification).
	\item Describe your protocol: a new state space model on the last layer, trained with SMC.
\end{itemize}

\section{Background}
\label{sec:background}

\subsection{Recurrent neural networks}
\label{sec:background:rnn}
Recurrent Neural Network (RNN) were first introduced as a more suited architecture for dealing with time varying input patterns \cite{Mozer1989AFB}.
By replacing buffer based approaches with an updated context state, RNN are able to solve time series problems with short time dependencies, but are lackluster in problems requiring long term memory due to vanishing and exploding gradient \cite{Bengio1994LearningLD}.
The Long Short Term Memory (LSTM) model proposed in \cite{Hochreiter1997LongSM} aims at bridging that gap by enforcing error flow throughout time in the network.
The LSTM architecture was modified in \cite{Cho2014LearningPR} in order to simplify its implementation and improve computation times, resulting in a novel model called Gated Recurrent Unit (GRU).

These recurrent architectures are able to approximate complex nonlinear time series.
Their parameters are conveniently estimated using current deep learning frameworks, which implement gradient descent through an automated differentiation procedure.
% In parallel to these advances on recurrent architectures, Convolutional Neural Networks (CNN), rendered popular by \cite{Krizhevsky2012ImageNetCW} for image classification, have been adapted to time series problem.
% The approaches proposed in \cite{Jzefowicz2016ExploringTL,Kim2016CharacterAwareNL} outperformed traditional Natural Language Processing (NLP) models by replacing the embedding layer with a character-level convolutional layer.
% Following this idea, \cite{Oord2016WaveNetAG} considerably improved on the speech to text state of the art, by using dilated convolutions, increasing the receptive fields of WaveNet at each layer.
% One year later, \cite{Oord2017ParallelWF} improved on the existing architecture by introducing Parallel WaveNet, which provided similar performance for a lower computational cost.

In this model, we implement a variation on the RNN, whose hidden states are computed as:
$$
	h_t = \tanh(W_h h_{t-1} + W_u u_t + b), \forall 1 \leq t \leq T
$$
where $u_t$ are the input at time $t$ and $\{W_u, W_h, b\}$ are the parameters of the model. The initial hidden value is often set to zero, $h_0 \equiv 0$.

\begin{itemize}
	\item insister sur souvent multi couches
	\item Insister sur la présence d'un etat caché pour toutes les archi récurrentes
\end{itemize}


% Recurrent and convolutional approaches coincide in that temporally close time steps data are matched together.
% In 2017, \cite{Vaswani2017AttentionIA}  proposed an attention based approach to solve NLP tasks that consider the entire input sequence in parallel.
% The Transformer model is based on a self-attention mechanism, that computes an attention value for every element of a sequence with respect to all others to model their dependency.
% This attention mechanism allows to understand at each time step which input elements are crucial to predict the new state.
% This makes these networks more interpretable than their most widely-used recurrent counterparts such as LSTM or GRU networks and motivates a keen interest for such approaches to predict complex time series.

\subsection{Sequential Monte Carlo methods}
\label{sec:background:smc}
\begin{itemize}
	\item Provide a generic introduction to SMC.
	\item Explain the use of SMC to compute gradients with Fisher's identity (gradient descent and EM).
\end{itemize}

\section{Last layer decoupling}
\label{sec:decoupling}
Our proposed architecture can be decomposed in two decoupled blocks.
An input model is responsible for extracting relevant features from the input time series, and can assume any architecture.
Its final hidden states are fed to a Sequential Monte Carlo Layer (SMCL), based on a plain RNN, for uncertainty estimation.

Considering an input model $f_\varphi$, our model is defined as follow:
\begin{equation*}
	\left\{
	\begin{aligned}
		\tilde u_h & = f_\varphi(u_t)                                     \\
		h_t        & = f_\theta(h_{t-1}, \tilde u_t, \eta_t) \\
		y_t        & = W_y h_t + b_y + \epsilon_t                         \\
	\end{aligned}
	\right.
\end{equation*}

Where $\eta$ and $\epsilon$ are two sequences of i.i.d. real valued noises with covariance matrices $\Sigma_x$ and $\Sigma_y$.

Provide simple equations for the smc model, along with formulas for the $\Sigma_{x, y}$ updates.

Protocol:
\begin{enumerate}
	\item Training with classic methods (train dataset)
	\item Finetuning with MC (train dataset)
\end{enumerate}

\begin{itemize}
	\item Un par introduction state model
	\item par description methodes monte carlo --> background ?
	\item precision pour notre algo
\end{itemize}

\section{Experiments}
\label{sec:exp}

\subsection{Training}%
\label{sub:training}

\begin{itemize}
	\item Moins important ; pas besoin de courbes
	\item Compare EM and gradient training for $\Sigma_{x, y}$ estimation
\end{itemize}

\subsection{Visualizations}%
\label{sub:visualizations}

Visualizations listed in priority order
\begin{enumerate}
	\item Prediction at t+1: plot t+1 predictions as boxplots, along with the mean and the observations.
	\item Prediction at t+k
	\item Compare weights evolution with classic finetuning
	\item Smoothed predictions: Smoothed particles trajectories' associated prediction: sample particles from the posterior, smooth the trajectories, apply the model function, and plot the results as a interval.
\end{enumerate}

\subsection{Evaluations}%
\label{sub:evaluations}

Evaluations listed in priority order
\begin{enumerate}
	\item Compare confidence interval with MC-dropout model (soit LSTM dropout classique, soit en enlevant la derniere couche)
	\item Compare with ARIMA/classic stat model
	\item Compare MSE (average on particles) with classic finetuning: sample particles for half a week. For the second half, observations are not available at all ; thus we either take the mean of the predictions associated with each particles, or sample from the observation model and take the mean.
	\item Compare linear SMC with kalman filter, sample under the gaussian law, plot boxplot
\end{enumerate}


% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
%\begin{figure}[htb]

%\begin{minipage}[b]{1.0\linewidth}
%  \centering
%  \centerline{\includegraphics[width=8.5cm]{image1}}
%%  \vspace{2.0cm}
%  \centerline{(a) Result 1}\medskip
%\end{minipage}
%%
%\begin{minipage}[b]{.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image3}}
%%  \vspace{1.5cm}
%  \centerline{(b) Results 3}\medskip
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image4}}
%%  \vspace{1.5cm}
%  \centerline{(c) Result 4}\medskip
%\end{minipage}
%%
%\caption{Example of placing a figure with experimental results.}
%\label{fig:res}
%%
%\end{figure}


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs.bib}

\end{document}

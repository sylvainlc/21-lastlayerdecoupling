% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx,xcolor}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage[ruled,vlined]{algorithm2e}

\title{Last layer state space model}
%
% Single address.
% ---------------
\name{Author(s) Name(s)\thanks{Thanks to XYZ agency for funding.}}
\address{Author Affiliation(s)}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%

\begin{document}
\maketitle

\begin{abstract}
	As sequential neural architectures become deeper and more complex, estimating the uncertainty of their predictions is ever so challenging.
	Efforts in quantifying uncertainty often rely on specific training procedures, and bear additional computational costs due to the dimensionality of such models.
	In this paper, we demonstrate how uncertainty can be estimated on top of an existing and trained neural network, combining a state space-based last  layer and  Sequential Monte Carlo methods. This approach allows to separate representation learning and uncertainty quantification. We apply our proposed methodology for the estimation of air quality in office buildings, through the hourly prediction of \ensuremath{\mathrm{CO_2}} and humidity levels.
	Our model accounts for the noisy data structure, due to unknown or unavailable variables (occupancy of the building, manual ventilation, etc.), and is able to provide confidence intervals on \ensuremath{\mathrm{CO_2}} predictions.
	\textcolor{red}{Plutôt à mettre l'introduction :  We believe a deeper understanding of \ensuremath{\mathrm{CO_2}} variations can assist in regulating and reducing HVAC consumption, while improving well being.}
\end{abstract}

\begin{keywords}
	One, two, three, four, five
\end{keywords}

\section{Introduction}
\label{sec:intro}

Recurrent Neural Networks (RNN) were first introduced as an efficient and convenient architecture to address short time dependencies problems \cite{Mozer1989AFB}.
They have been consistently improved to develop longer term memory, and optimize their implementations \cite{Bengio1994LearningLD,Hochreiter1997LongSM,Cho2014LearningPR}.
Current deep learning framework allows stacking arbitrary high number of recurrent layers, whose parameters are estimated by gradient descent through an automated differentiation procedure.

These recurrent architectures are able to approximate complex nonlinear time series, but suffer from overconfidence when evaluated outside of their training observations. Bayesian statistics aim at mitigating this drawback by providing uncertainty estimation \cite{Hinton1995BayesianLF,MacKay1992}.

Several architectures inspired by variational inference emerged by considering latent states as random variables and approximating the posterior.
The authors of \cite{Chung2015NIPS} built on a traditional RNN architecture by modelling temporal dependencies between these latent random states.
Results presented in \cite{Fortunato2017bayesian} yield improved performances when considering local gradient information for computing the posterior.
Similarly, introducing random variables directly into a neural network's weights can mitigate overconfidence, as demonstrated by the authors of \cite{Hinton1993}.
In \cite{Blundell2015}, weights are considered as distributions instead of vector values, allowing the network to make more reasonable predictions about unseen data.

Variational methods display encouraging results for modelling uncertainty in existing models, but require significant alteration of the model, as well as it's training mechanism.
In contrast, Monte Carlo Dropout (MC Dropout) methods offer to capture uncertainty by adding a single non parametric Dropout layer during both training and evaluation tasks, producing variable predictions from a single trained model, see \cite{Gal2016}.
These results were later extended to recurrent architectures in \cite{Gal2016NIPS}.
In the following years, MC Dropout methods have been applied in many industrial fields, such as anomaly detection (\cite{Zhu2017DeepAC}), flight delay prediction (\cite{Vandal2018}) or molecular simulations (\cite{Wen2020UncertaintyQI}).
Alternatively, ensemble methods consists in training distinct networks to obtain a combined prediction, as shown in \cite{Pearce2018}.
However, these frequentist approaches fail to guarantee proper calibration of the model, as highlighted by \cite{ashukha2020pitfalls}, and suffer various limitations, see \cite{Fong2020}.

In this paper, we introduce a new method for uncertainty estimation combining high expressivity, quality uncertainty estimations and ease of training.
Our proposed architecture is composed of an arbitrary sequential model, followed by a decoupled state space model layer.
The former can be fitted through a traditional gradient descent iteration, while the latter is trained using Sequential Monte Carlo methods.
Our uncertainty estimation layer can thus be built on top of an existing, already trained model.


\section{Last layer decoupling}
\label{sec:decoupling}

\subsection{Proposed architecture}%
\label{sub:proposed_architecture}

Our proposed architecture can be decomposed in two decoupled blocks.
An input model $h_\varphi$ is responsible for extracting relevant features from the input time series.
Its final hidden states are fed to a Sequential Monte Carlo Layer (SMCL), based on a plain RNN, for uncertainty estimation.

We consider the regression task associated with observations $(y_1, \cdots y_T)$ and inputs $(u_1, \cdots, u_T)$, and denote by $(x_{1:T})$ the hidden states of the model.
\begin{equation*}
	\left\{
	\begin{aligned}
		\tilde u_t & = h_\varphi(u_t)                        & \text{input model}       \\
		x_t        & = g_\theta(x_{t-1}, \tilde u_t, \eta_t) & \text{state model}       \\
		y_t        & = f_\theta(x_t, \epsilon_t)             & \text{observation model} \\
	\end{aligned}
	\right.
\end{equation*}
Where $\eta$ and $\epsilon$ are two sequences of i.i.d. real valued noises with covariance matrices $\Sigma_x$ and $\Sigma_y$.

\subsection{Input model}%
\label{sub:input_model}
We chose a multi layered GRU network to extract high level features from the inputs.
Each hidden state is computed as follows for layer $1 \leq l \leq L$:
$$
	\tilde u^l_t = k_\varphi^l(\tilde u^{l-1}_t, \tilde u^l_{t-1}) \quad \forall 1 \leq t \leq T
$$
where the first layer is assimilated to the input vectors, $\tilde u^0 \equiv u$, and the initial hidden value is set to zero, $\tilde u^l_0 \equiv 0$.
This model in trained by disabling the noise addition in the SMCL functions $g_\theta$ and $f_\theta$, and considering the entire model as a deterministic neural network.
Through a traditional gradient descent, we then obtain estimations for the input model weights $\varphi$, as well as a pretraining for the SMCL layer.

\subsection{Uncertainty estimation}%
\label{sub:uncertainty_estimation}

We account for uncertainty in the pretrained state and observation models by considering hidden states as random variables, and estimating their parameters.
Consequently, we can no longer integrate over their possible values, and computation of the gradient becomes intractable.
Instead, we leverage Fisher's identity for an expression involving the posterior conditional expectation.

\begin{align}
	\nabla \log p_\theta(Y_{1:T}) = \mathbb{E}_\theta \left[ \nabla\log p_\theta(X_{1:T}, Y_{1:T}) | Y_{1:T} \right]
	\label{eq:grad_ll}
\end{align}

Sequential Monte Carlo methods aim at approximating this posterior by a set of $N$ weighted random trajectories.
At each time step $t$, these particles $\xi^{1:N}_t$ are associated likelihood weights $\omega^{1:N}_t$, given the previous observation $y_{t-1}$, in order to propagate only the most likely candidates, see \ref{algo:particle_filter}.
For any function $h$, we can approximate the conditional expectation as follows:
\begin{align*}
	\mathbb{E}_\theta \left[ h(X_{1:T}) | Y_{1:T} \right] = \sum_{i=1}^N \omega_T^i h(\xi^i_{1:T})
\end{align*}

We develop \ref{eq:grad_ll} to obtain an explicit form for loss function ; its gradient is an approximation for the likelihood's gradient, and can be computed by deep learning frameworks through automated differentiation.
\begin{align*}
	\mathbb{J}(\theta) & = \log |\Sigma_x| + \log |\Sigma_y|                                                                                                        \\
	                   & + \frac{1}{T}\sum_{k=0}^T \sum_{i=1}^N \omega^i (y_k - f_\theta(\xi_k^i))' \Sigma_y^{-1} (y_k - f_\theta(\xi_k^i))                         \\
	                   & + \frac{1}{T}\sum_{k=1}^T \sum_{i=1}^N \omega^i (\xi_k^i - g_\theta(\xi_{k-1}^i, u_k))'\Sigma_x^{-1}(\xi_k^i - g_\theta(\xi_{k-1}^i, u_k))
\end{align*}

During the estimation of $\theta$, the input model is frozen, i.e. we stop gradient computation for $\varphi$. Both $\Sigma_x$ and $\Sigma_y$ are updated by an explicit EM, as gradient descent for these parameters proved unstable.

\begin{algorithm}
	\KwIn{$\theta, y_{1:T}, \tilde u_{1:T}$}
	\KwOut{$\xi_{1:T}^{1:N}$}
	$\xi_0^i \gets \eta^i$\;
	\For{$t \gets 0$ \KwTo $T$}{
		$\omega_t^i \gets \varphi_{y_t, \Sigma_y}(f_\theta(\xi_t^i))$\;
		$I_{t+1}^i \sim \mathbb{P}(I_{t+1}^i=j) = \omega_t^j$\;
		$\xi_{t+1}^i \gets g_\theta(\xi_t^{I_{t+1}^i}, \tilde u_{t+1}, \eta^i)$\;
	}
	\caption{Particle filter. $\varphi_{\mu, \Sigma}$ is the multivariate density function with mean $\mu$ and covariance matrix $\Sigma$.}
	\label{algo:particle_filter}
\end{algorithm}

\section{Experiments}
\label{sec:exp}

\subsection{Training}%
\label{sub:training}

\begin{itemize}
	\item Moins important ; pas besoin de courbes
	\item Compare EM and gradient training for $\Sigma_{x, y}$ estimation
\end{itemize}

\subsection{Visualizations}%
\label{sub:visualizations}

Visualizations listed in priority order
\begin{enumerate}
	\item Prediction at t+1: plot t+1 predictions as boxplots, along with the mean and the observations.
	\item Prediction at t+k
	\item Compare weights evolution with classic finetuning
	\item Smoothed predictions: Smoothed particles trajectories' associated prediction: sample particles from the posterior, smooth the trajectories, apply the model function, and plot the results as a interval.
\end{enumerate}

\subsection{Evaluations}%
\label{sub:evaluations}

Evaluations listed in priority order
\begin{enumerate}
	\item Compare confidence interval with MC-dropout model (soit LSTM dropout classique, soit en enlevant la derniere couche)
	\item Compare with ARIMA/classic stat model
	\item Compare MSE (average on particles) with classic finetuning: sample particles for half a week. For the second half, observations are not available at all ; thus we either take the mean of the predictions associated with each particles, or sample from the observation model and take the mean.
	\item Compare linear SMC with kalman filter, sample under the gaussian law, plot boxplot
\end{enumerate}


% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
%\begin{figure}[htb]

%\begin{minipage}[b]{1.0\linewidth}
%  \centering
%  \centerline{\includegraphics[width=8.5cm]{image1}}
%%  \vspace{2.0cm}
%  \centerline{(a) Result 1}\medskip
%\end{minipage}
%%
%\begin{minipage}[b]{.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image3}}
%%  \vspace{1.5cm}
%  \centerline{(b) Results 3}\medskip
%\end{minipage}
%\hfill
%\begin{minipage}[b]{0.48\linewidth}
%  \centering
%  \centerline{\includegraphics[width=4.0cm]{image4}}
%%  \vspace{1.5cm}
%  \centerline{(c) Result 4}\medskip
%\end{minipage}
%%
%\caption{Example of placing a figure with experimental results.}
%\label{fig:res}
%%
%\end{figure}


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs.bib}

\end{document}
